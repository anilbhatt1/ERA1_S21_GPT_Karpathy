{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRfjkC8a6OG1",
        "outputId": "a056bc40-426b-45f1-b8f6-5cb39bc2884f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 26 09:38:54 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "x9XQbuyE50VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQupgWAyUqT-",
        "outputId": "30f23fc4-b07d-422f-a50e-643ebba41cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 64 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1ZE8rvC510M",
        "outputId": "fec74b45-eb9b-4258-cb3b-70a67efb4b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79454c2f0190>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "print(f'vocab_size : {vocab_size}, len(data) : {len(data)}')\n",
        "print(f\"chars is {''.join(chars)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCcnnUh8551Y",
        "outputId": "7a3f6a8e-55ec-4f51-c96b-c4f7d5a4870e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size : 95, len(data) : 1490469\n",
            "chars is \n",
            " !\"$&'()*,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]abcdefghijklmnopqrstuvwxyz~£é–—‘’‚“”…™\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(chars), len(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkueExxPJtP0",
        "outputId": "a1218fcb-107c-45e4-8cbf-ec494699d3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, 95)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "paaDbysV6Aw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# gpt model\n",
        "class gptModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "yz7ivOEM6Qho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = gptModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e3, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-lsnVmX6RkD",
        "outputId": "2a7f1eb1-131a-4f9f-eee0-36f2ea559406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215.647 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "d1oIl-S5lzg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a18f4706-0808-49aa-9a77-ab7eca8da018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6837, val loss 4.6925\n",
            "step 0: loss 4.6928391456604\n",
            "step 10: loss 3.4680612087249756\n",
            "step 20: loss 3.2378900051116943\n",
            "step 30: loss 3.1328256130218506\n",
            "step 40: loss 3.029555082321167\n",
            "step 50: loss 2.8653759956359863\n",
            "step 60: loss 2.8849446773529053\n",
            "step 70: loss 2.7572507858276367\n",
            "step 80: loss 2.855921983718872\n",
            "step 90: loss 2.8179965019226074\n",
            "step 100: train loss 2.7248, val loss 2.7306\n",
            "step 100: loss 2.7240850925445557\n",
            "step 110: loss 2.7100865840911865\n",
            "step 120: loss 2.6172914505004883\n",
            "step 130: loss 2.662379741668701\n",
            "step 140: loss 2.6295323371887207\n",
            "step 150: loss 2.6104109287261963\n",
            "step 160: loss 2.6635866165161133\n",
            "step 170: loss 2.58577299118042\n",
            "step 180: loss 2.663145065307617\n",
            "step 190: loss 2.6330981254577637\n",
            "step 200: train loss 2.6226, val loss 2.6300\n",
            "step 200: loss 2.635530948638916\n",
            "step 210: loss 2.624163866043091\n",
            "step 220: loss 2.608628749847412\n",
            "step 230: loss 2.6513617038726807\n",
            "step 240: loss 2.568615436553955\n",
            "step 250: loss 2.5965194702148438\n",
            "step 260: loss 2.6931986808776855\n",
            "step 270: loss 2.6245670318603516\n",
            "step 280: loss 2.543220281600952\n",
            "step 290: loss 2.586780309677124\n",
            "step 300: train loss 2.5605, val loss 2.5822\n",
            "step 300: loss 2.5224623680114746\n",
            "step 310: loss 2.6923272609710693\n",
            "step 320: loss 2.576988458633423\n",
            "step 330: loss 2.5437874794006348\n",
            "step 340: loss 2.5232014656066895\n",
            "step 350: loss 2.5189483165740967\n",
            "step 360: loss 2.4841556549072266\n",
            "step 370: loss 2.478240728378296\n",
            "step 380: loss 2.5113730430603027\n",
            "step 390: loss 2.5445377826690674\n",
            "step 400: train loss 2.5267, val loss 2.5311\n",
            "step 400: loss 2.550522804260254\n",
            "step 410: loss 2.515223503112793\n",
            "step 420: loss 2.485729694366455\n",
            "step 430: loss 2.5002999305725098\n",
            "step 440: loss 2.489912509918213\n",
            "step 450: loss 2.4407846927642822\n",
            "step 460: loss 2.5057790279388428\n",
            "step 470: loss 2.5195372104644775\n",
            "step 480: loss 2.6203596591949463\n",
            "step 490: loss 2.4726102352142334\n",
            "step 500: train loss 2.4789, val loss 2.4864\n",
            "step 500: loss 2.55222487449646\n",
            "step 510: loss 2.50809645652771\n",
            "step 520: loss 2.5634243488311768\n",
            "step 530: loss 2.415989637374878\n",
            "step 540: loss 2.4307339191436768\n",
            "step 550: loss 2.4559383392333984\n",
            "step 560: loss 2.413583517074585\n",
            "step 570: loss 2.523129463195801\n",
            "step 580: loss 2.437110185623169\n",
            "step 590: loss 2.3867619037628174\n",
            "step 600: train loss 2.4277, val loss 2.4327\n",
            "step 600: loss 2.469208002090454\n",
            "step 610: loss 2.3938844203948975\n",
            "step 620: loss 2.3002772331237793\n",
            "step 630: loss 2.4237008094787598\n",
            "step 640: loss 2.321324348449707\n",
            "step 650: loss 2.4426941871643066\n",
            "step 660: loss 2.3651211261749268\n",
            "step 670: loss 2.381479024887085\n",
            "step 680: loss 2.366544008255005\n",
            "step 690: loss 2.332855701446533\n",
            "step 700: train loss 2.3573, val loss 2.3724\n",
            "step 700: loss 2.2789859771728516\n",
            "step 710: loss 2.368697166442871\n",
            "step 720: loss 2.3983237743377686\n",
            "step 730: loss 2.3254637718200684\n",
            "step 740: loss 2.3986520767211914\n",
            "step 750: loss 2.295131206512451\n",
            "step 760: loss 2.3277480602264404\n",
            "step 770: loss 2.3333933353424072\n",
            "step 780: loss 2.3069474697113037\n",
            "step 790: loss 2.342536687850952\n",
            "step 800: train loss 2.3017, val loss 2.3085\n",
            "step 800: loss 2.3230295181274414\n",
            "step 810: loss 2.3931615352630615\n",
            "step 820: loss 2.273233652114868\n",
            "step 830: loss 2.243947982788086\n",
            "step 840: loss 2.179295778274536\n",
            "step 850: loss 2.2399747371673584\n",
            "step 860: loss 2.2428433895111084\n",
            "step 870: loss 2.2621607780456543\n",
            "step 880: loss 2.323746919631958\n",
            "step 890: loss 2.255666971206665\n",
            "step 900: train loss 2.2489, val loss 2.2535\n",
            "step 900: loss 2.3014907836914062\n",
            "step 910: loss 2.17948055267334\n",
            "step 920: loss 2.3253986835479736\n",
            "step 930: loss 2.2504677772521973\n",
            "step 940: loss 2.315458059310913\n",
            "step 950: loss 2.2174856662750244\n",
            "step 960: loss 2.2194817066192627\n",
            "step 970: loss 2.278162717819214\n",
            "step 980: loss 2.1249589920043945\n",
            "step 990: loss 2.1875293254852295\n",
            "step 1000: train loss 2.2021, val loss 2.2009\n",
            "step 1000: loss 2.132753849029541\n",
            "step 1010: loss 2.3310227394104004\n",
            "step 1020: loss 2.2636830806732178\n",
            "step 1030: loss 2.264887571334839\n",
            "step 1040: loss 2.191004514694214\n",
            "step 1050: loss 2.1328201293945312\n",
            "step 1060: loss 2.2386255264282227\n",
            "step 1070: loss 2.27089524269104\n",
            "step 1080: loss 2.0619304180145264\n",
            "step 1090: loss 2.0504109859466553\n",
            "step 1100: train loss 2.1596, val loss 2.1608\n",
            "step 1100: loss 2.2694475650787354\n",
            "step 1110: loss 2.098604679107666\n",
            "step 1120: loss 2.13789439201355\n",
            "step 1130: loss 2.1904542446136475\n",
            "step 1140: loss 2.1223561763763428\n",
            "step 1150: loss 2.1677236557006836\n",
            "step 1160: loss 2.215050220489502\n",
            "step 1170: loss 2.1661839485168457\n",
            "step 1180: loss 2.31213641166687\n",
            "step 1190: loss 2.0975418090820312\n",
            "step 1200: train loss 2.1242, val loss 2.1320\n",
            "step 1200: loss 2.0958237648010254\n",
            "step 1210: loss 2.0907936096191406\n",
            "step 1220: loss 2.18493914604187\n",
            "step 1230: loss 2.1962625980377197\n",
            "step 1240: loss 2.0728089809417725\n",
            "step 1250: loss 2.073230028152466\n",
            "step 1260: loss 2.1438872814178467\n",
            "step 1270: loss 2.1483473777770996\n",
            "step 1280: loss 2.101525068283081\n",
            "step 1290: loss 2.0956413745880127\n",
            "step 1300: train loss 2.0824, val loss 2.0818\n",
            "step 1300: loss 2.157388210296631\n",
            "step 1310: loss 2.19008731842041\n",
            "step 1320: loss 1.966894268989563\n",
            "step 1330: loss 2.1443631649017334\n",
            "step 1340: loss 2.101191997528076\n",
            "step 1350: loss 2.0497355461120605\n",
            "step 1360: loss 2.107149839401245\n",
            "step 1370: loss 2.000901937484741\n",
            "step 1380: loss 2.1086511611938477\n",
            "step 1390: loss 2.0729823112487793\n",
            "step 1400: train loss 2.0579, val loss 2.0557\n",
            "step 1400: loss 2.0559163093566895\n",
            "step 1410: loss 2.124663829803467\n",
            "step 1420: loss 1.990500569343567\n",
            "step 1430: loss 2.0702881813049316\n",
            "step 1440: loss 1.9798554182052612\n",
            "step 1450: loss 1.9989863634109497\n",
            "step 1460: loss 2.037010908126831\n",
            "step 1470: loss 2.01434063911438\n",
            "step 1480: loss 2.1078896522521973\n",
            "step 1490: loss 2.021756172180176\n",
            "step 1500: train loss 2.0363, val loss 2.0367\n",
            "step 1500: loss 2.037297487258911\n",
            "step 1510: loss 1.99490487575531\n",
            "step 1520: loss 2.0495121479034424\n",
            "step 1530: loss 2.0037097930908203\n",
            "step 1540: loss 2.021110773086548\n",
            "step 1550: loss 2.183145523071289\n",
            "step 1560: loss 1.9990612268447876\n",
            "step 1570: loss 2.0079972743988037\n",
            "step 1580: loss 1.9107378721237183\n",
            "step 1590: loss 2.0117053985595703\n",
            "step 1600: train loss 2.0089, val loss 2.0093\n",
            "step 1600: loss 2.0250439643859863\n",
            "step 1610: loss 2.104264974594116\n",
            "step 1620: loss 1.9411919116973877\n",
            "step 1630: loss 2.0644030570983887\n",
            "step 1640: loss 1.9679372310638428\n",
            "step 1650: loss 1.9592342376708984\n",
            "step 1660: loss 2.011070728302002\n",
            "step 1670: loss 1.9248344898223877\n",
            "step 1680: loss 2.060720205307007\n",
            "step 1690: loss 2.0532803535461426\n",
            "step 1700: train loss 1.9777, val loss 1.9835\n",
            "step 1700: loss 1.9195924997329712\n",
            "step 1710: loss 2.0686023235321045\n",
            "step 1720: loss 1.9535459280014038\n",
            "step 1730: loss 1.9345613718032837\n",
            "step 1740: loss 1.8983920812606812\n",
            "step 1750: loss 1.8754384517669678\n",
            "step 1760: loss 1.9078422784805298\n",
            "step 1770: loss 1.9821146726608276\n",
            "step 1780: loss 1.9767979383468628\n",
            "step 1790: loss 1.9203906059265137\n",
            "step 1800: train loss 1.9764, val loss 1.9766\n",
            "step 1800: loss 2.03157901763916\n",
            "step 1810: loss 1.9754379987716675\n",
            "step 1820: loss 1.9390040636062622\n",
            "step 1830: loss 2.0313429832458496\n",
            "step 1840: loss 1.973002552986145\n",
            "step 1850: loss 1.9490272998809814\n",
            "step 1860: loss 1.9711335897445679\n",
            "step 1870: loss 1.9870153665542603\n",
            "step 1880: loss 1.9387034177780151\n",
            "step 1890: loss 1.9224244356155396\n",
            "step 1900: train loss 1.9492, val loss 1.9536\n",
            "step 1900: loss 1.898121953010559\n",
            "step 1910: loss 1.968936800956726\n",
            "step 1920: loss 1.9788882732391357\n",
            "step 1930: loss 1.9504817724227905\n",
            "step 1940: loss 1.9841331243515015\n",
            "step 1950: loss 2.02776837348938\n",
            "step 1960: loss 1.8558305501937866\n",
            "step 1970: loss 1.849049687385559\n",
            "step 1980: loss 1.8849447965621948\n",
            "step 1990: loss 1.838889718055725\n",
            "step 2000: train loss 1.9378, val loss 1.9413\n",
            "step 2000: loss 1.847123146057129\n",
            "step 2010: loss 1.8702493906021118\n",
            "step 2020: loss 1.928035020828247\n",
            "step 2030: loss 2.005584478378296\n",
            "step 2040: loss 1.9279899597167969\n",
            "step 2050: loss 1.9238022565841675\n",
            "step 2060: loss 1.9421718120574951\n",
            "step 2070: loss 1.8279017210006714\n",
            "step 2080: loss 1.8454320430755615\n",
            "step 2090: loss 1.9484423398971558\n",
            "step 2100: train loss 1.9170, val loss 1.9175\n",
            "step 2100: loss 1.9347065687179565\n",
            "step 2110: loss 1.9605649709701538\n",
            "step 2120: loss 1.90489661693573\n",
            "step 2130: loss 1.9883203506469727\n",
            "step 2140: loss 1.9043419361114502\n",
            "step 2150: loss 1.8653541803359985\n",
            "step 2160: loss 1.8730288743972778\n",
            "step 2170: loss 1.8044517040252686\n",
            "step 2180: loss 1.946710228919983\n",
            "step 2190: loss 1.757598876953125\n",
            "step 2200: train loss 1.9155, val loss 1.9121\n",
            "step 2200: loss 1.9338386058807373\n",
            "step 2210: loss 1.7815520763397217\n",
            "step 2220: loss 1.8945320844650269\n",
            "step 2230: loss 1.9743576049804688\n",
            "step 2240: loss 1.9072540998458862\n",
            "step 2250: loss 1.8498432636260986\n",
            "step 2260: loss 1.8820161819458008\n",
            "step 2270: loss 1.8827314376831055\n",
            "step 2280: loss 1.8510295152664185\n",
            "step 2290: loss 1.8423274755477905\n",
            "step 2300: train loss 1.8776, val loss 1.8672\n",
            "step 2300: loss 1.8626328706741333\n",
            "step 2310: loss 1.825944185256958\n",
            "step 2320: loss 1.8903709650039673\n",
            "step 2330: loss 1.8570114374160767\n",
            "step 2340: loss 1.9025163650512695\n",
            "step 2350: loss 1.8783124685287476\n",
            "step 2360: loss 1.885342001914978\n",
            "step 2370: loss 1.9005990028381348\n",
            "step 2380: loss 1.8715591430664062\n",
            "step 2390: loss 1.922037959098816\n",
            "step 2400: train loss 1.8658, val loss 1.8652\n",
            "step 2400: loss 1.7702081203460693\n",
            "step 2410: loss 2.009122371673584\n",
            "step 2420: loss 1.7973984479904175\n",
            "step 2430: loss 1.9213756322860718\n",
            "step 2440: loss 1.788198709487915\n",
            "step 2450: loss 1.9150612354278564\n",
            "step 2460: loss 1.7659153938293457\n",
            "step 2470: loss 1.9571908712387085\n",
            "step 2480: loss 1.8365172147750854\n",
            "step 2490: loss 1.848090410232544\n",
            "step 2500: train loss 1.8472, val loss 1.8544\n",
            "step 2500: loss 1.8872687816619873\n",
            "step 2510: loss 1.8559718132019043\n",
            "step 2520: loss 1.7477790117263794\n",
            "step 2530: loss 1.9452091455459595\n",
            "step 2540: loss 1.936008334159851\n",
            "step 2550: loss 1.8321192264556885\n",
            "step 2560: loss 1.8614308834075928\n",
            "step 2570: loss 1.9533493518829346\n",
            "step 2580: loss 1.8946337699890137\n",
            "step 2590: loss 1.8190909624099731\n",
            "step 2600: train loss 1.8406, val loss 1.8332\n",
            "step 2600: loss 1.8478292226791382\n",
            "step 2610: loss 1.873449444770813\n",
            "step 2620: loss 1.7887119054794312\n",
            "step 2630: loss 1.8357590436935425\n",
            "step 2640: loss 1.8804038763046265\n",
            "step 2650: loss 1.950499415397644\n",
            "step 2660: loss 1.8593908548355103\n",
            "step 2670: loss 1.7345997095108032\n",
            "step 2680: loss 1.7807315587997437\n",
            "step 2690: loss 1.9851195812225342\n",
            "step 2700: train loss 1.8315, val loss 1.8293\n",
            "step 2700: loss 1.9098669290542603\n",
            "step 2710: loss 1.9631123542785645\n",
            "step 2720: loss 1.8939671516418457\n",
            "step 2730: loss 1.7931804656982422\n",
            "step 2740: loss 1.7135792970657349\n",
            "step 2750: loss 1.8318150043487549\n",
            "step 2760: loss 1.8170418739318848\n",
            "step 2770: loss 1.9044941663742065\n",
            "step 2780: loss 1.8446524143218994\n",
            "step 2790: loss 1.8034969568252563\n",
            "step 2800: train loss 1.8258, val loss 1.8146\n",
            "step 2800: loss 1.799346923828125\n",
            "step 2810: loss 1.8730295896530151\n",
            "step 2820: loss 1.8140721321105957\n",
            "step 2830: loss 1.8538477420806885\n",
            "step 2840: loss 1.8740031719207764\n",
            "step 2850: loss 1.9160089492797852\n",
            "step 2860: loss 1.8054183721542358\n",
            "step 2870: loss 1.8001668453216553\n",
            "step 2880: loss 1.7600398063659668\n",
            "step 2890: loss 1.8787307739257812\n",
            "step 2900: train loss 1.8112, val loss 1.8121\n",
            "step 2900: loss 1.9017876386642456\n",
            "step 2910: loss 1.8588076829910278\n",
            "step 2920: loss 1.7758598327636719\n",
            "step 2930: loss 1.8773512840270996\n",
            "step 2940: loss 1.8227853775024414\n",
            "step 2950: loss 1.83217453956604\n",
            "step 2960: loss 1.7622675895690918\n",
            "step 2970: loss 1.7991503477096558\n",
            "step 2980: loss 1.8100495338439941\n",
            "step 2990: loss 1.8218783140182495\n",
            "step 3000: train loss 1.7941, val loss 1.7899\n",
            "step 3000: loss 1.8088399171829224\n",
            "step 3010: loss 1.8183335065841675\n",
            "step 3020: loss 1.7352138757705688\n",
            "step 3030: loss 1.8349984884262085\n",
            "step 3040: loss 2.0026791095733643\n",
            "step 3050: loss 1.8260859251022339\n",
            "step 3060: loss 1.726251244544983\n",
            "step 3070: loss 1.7720954418182373\n",
            "step 3080: loss 1.8823970556259155\n",
            "step 3090: loss 1.699697732925415\n",
            "step 3100: train loss 1.7924, val loss 1.7834\n",
            "step 3100: loss 1.7116066217422485\n",
            "step 3110: loss 1.8646254539489746\n",
            "step 3120: loss 1.8488248586654663\n",
            "step 3130: loss 1.7739957571029663\n",
            "step 3140: loss 1.773237943649292\n",
            "step 3150: loss 1.7945556640625\n",
            "step 3160: loss 1.800451397895813\n",
            "step 3170: loss 1.7926183938980103\n",
            "step 3180: loss 1.8813141584396362\n",
            "step 3190: loss 1.7413227558135986\n",
            "step 3200: train loss 1.7755, val loss 1.7830\n",
            "step 3200: loss 1.789788007736206\n",
            "step 3210: loss 1.8039711713790894\n",
            "step 3220: loss 1.820844054222107\n",
            "step 3230: loss 1.8267861604690552\n",
            "step 3240: loss 1.8097171783447266\n",
            "step 3250: loss 1.6358933448791504\n",
            "step 3260: loss 1.848549723625183\n",
            "step 3270: loss 1.7580074071884155\n",
            "step 3280: loss 1.8389390707015991\n",
            "step 3290: loss 1.7771333456039429\n",
            "step 3300: train loss 1.7667, val loss 1.7712\n",
            "step 3300: loss 1.8764511346817017\n",
            "step 3310: loss 1.6891028881072998\n",
            "step 3320: loss 1.8166512250900269\n",
            "step 3330: loss 1.7529735565185547\n",
            "step 3340: loss 1.8315987586975098\n",
            "step 3350: loss 1.532517910003662\n",
            "step 3360: loss 1.7202539443969727\n",
            "step 3370: loss 1.8198018074035645\n",
            "step 3380: loss 1.8228492736816406\n",
            "step 3390: loss 1.8723939657211304\n",
            "step 3400: train loss 1.7665, val loss 1.7632\n",
            "step 3400: loss 1.7143394947052002\n",
            "step 3410: loss 1.7077040672302246\n",
            "step 3420: loss 1.6890395879745483\n",
            "step 3430: loss 1.8128159046173096\n",
            "step 3440: loss 1.7225557565689087\n",
            "step 3450: loss 1.785302996635437\n",
            "step 3460: loss 1.682011365890503\n",
            "step 3470: loss 1.8917018175125122\n",
            "step 3480: loss 1.6278799772262573\n",
            "step 3490: loss 1.7555044889450073\n",
            "step 3500: train loss 1.7559, val loss 1.7567\n",
            "step 3500: loss 1.8084895610809326\n",
            "step 3510: loss 1.5992789268493652\n",
            "step 3520: loss 1.8344533443450928\n",
            "step 3530: loss 1.752402424812317\n",
            "step 3540: loss 1.8268072605133057\n",
            "step 3550: loss 1.797845482826233\n",
            "step 3560: loss 1.7296366691589355\n",
            "step 3570: loss 1.689308762550354\n",
            "step 3580: loss 1.8301886320114136\n",
            "step 3590: loss 1.7994697093963623\n",
            "step 3600: train loss 1.7417, val loss 1.7533\n",
            "step 3600: loss 1.6196904182434082\n",
            "step 3610: loss 1.7287838459014893\n",
            "step 3620: loss 1.7596979141235352\n",
            "step 3630: loss 1.7588261365890503\n",
            "step 3640: loss 1.656057357788086\n",
            "step 3650: loss 1.6925090551376343\n",
            "step 3660: loss 1.7204124927520752\n",
            "step 3670: loss 1.6564732789993286\n",
            "step 3680: loss 1.7778651714324951\n",
            "step 3690: loss 1.7722927331924438\n",
            "step 3700: train loss 1.7330, val loss 1.7347\n",
            "step 3700: loss 1.7533811330795288\n",
            "step 3710: loss 1.7577605247497559\n",
            "step 3720: loss 1.7346844673156738\n",
            "step 3730: loss 1.7538055181503296\n",
            "step 3740: loss 1.5502408742904663\n",
            "step 3750: loss 1.6249589920043945\n",
            "step 3760: loss 1.8446767330169678\n",
            "step 3770: loss 1.6959258317947388\n",
            "step 3780: loss 1.7281064987182617\n",
            "step 3790: loss 1.7645776271820068\n",
            "step 3800: train loss 1.7308, val loss 1.7391\n",
            "step 3800: loss 1.82013738155365\n",
            "step 3810: loss 1.8271853923797607\n",
            "step 3820: loss 1.7855769395828247\n",
            "step 3830: loss 1.715511441230774\n",
            "step 3840: loss 1.6144870519638062\n",
            "step 3850: loss 1.6429696083068848\n",
            "step 3860: loss 1.8511048555374146\n",
            "step 3870: loss 1.7932429313659668\n",
            "step 3880: loss 1.7471791505813599\n",
            "step 3890: loss 1.7346875667572021\n",
            "step 3900: train loss 1.7309, val loss 1.7240\n",
            "step 3900: loss 1.7569204568862915\n",
            "step 3910: loss 1.7286243438720703\n",
            "step 3920: loss 1.63406503200531\n",
            "step 3930: loss 1.7609049081802368\n",
            "step 3940: loss 1.600612759590149\n",
            "step 3950: loss 1.7211925983428955\n",
            "step 3960: loss 1.7139647006988525\n",
            "step 3970: loss 1.7357394695281982\n",
            "step 3980: loss 1.7701245546340942\n",
            "step 3990: loss 1.7390058040618896\n",
            "step 4000: train loss 1.7192, val loss 1.7170\n",
            "step 4000: loss 1.6879490613937378\n",
            "step 4010: loss 1.6175302267074585\n",
            "step 4020: loss 1.713388442993164\n",
            "step 4030: loss 1.7784900665283203\n",
            "step 4040: loss 1.7535663843154907\n",
            "step 4050: loss 1.708647608757019\n",
            "step 4060: loss 1.681101679801941\n",
            "step 4070: loss 1.7325512170791626\n",
            "step 4080: loss 1.6386487483978271\n",
            "step 4090: loss 1.8487697839736938\n",
            "step 4100: train loss 1.7181, val loss 1.7085\n",
            "step 4100: loss 1.7576086521148682\n",
            "step 4110: loss 1.744978904724121\n",
            "step 4120: loss 1.8363853693008423\n",
            "step 4130: loss 1.7443182468414307\n",
            "step 4140: loss 1.7213377952575684\n",
            "step 4150: loss 1.6755039691925049\n",
            "step 4160: loss 1.8374265432357788\n",
            "step 4170: loss 1.6967511177062988\n",
            "step 4180: loss 1.6648838520050049\n",
            "step 4190: loss 1.66317617893219\n",
            "step 4200: train loss 1.7045, val loss 1.7158\n",
            "step 4200: loss 1.7583531141281128\n",
            "step 4210: loss 1.7489651441574097\n",
            "step 4220: loss 1.7547857761383057\n",
            "step 4230: loss 1.7057619094848633\n",
            "step 4240: loss 1.735234022140503\n",
            "step 4250: loss 1.6887844800949097\n",
            "step 4260: loss 1.7022945880889893\n",
            "step 4270: loss 1.7274004220962524\n",
            "step 4280: loss 1.6997945308685303\n",
            "step 4290: loss 1.7762064933776855\n",
            "step 4300: train loss 1.7008, val loss 1.6972\n",
            "step 4300: loss 1.6580737829208374\n",
            "step 4310: loss 1.6428967714309692\n",
            "step 4320: loss 1.7096325159072876\n",
            "step 4330: loss 1.6718809604644775\n",
            "step 4340: loss 1.7212445735931396\n",
            "step 4350: loss 1.7372008562088013\n",
            "step 4360: loss 1.8889700174331665\n",
            "step 4370: loss 1.7889506816864014\n",
            "step 4380: loss 1.722493290901184\n",
            "step 4390: loss 1.6712100505828857\n",
            "step 4400: train loss 1.6944, val loss 1.7083\n",
            "step 4400: loss 1.5524108409881592\n",
            "step 4410: loss 1.7207982540130615\n",
            "step 4420: loss 1.773556113243103\n",
            "step 4430: loss 1.7457407712936401\n",
            "step 4440: loss 1.6549299955368042\n",
            "step 4450: loss 1.5223625898361206\n",
            "step 4460: loss 1.7980670928955078\n",
            "step 4470: loss 1.755483627319336\n",
            "step 4480: loss 1.6862541437149048\n",
            "step 4490: loss 1.7012007236480713\n",
            "step 4500: train loss 1.6812, val loss 1.6973\n",
            "step 4500: loss 1.705254316329956\n",
            "step 4510: loss 1.8023056983947754\n",
            "step 4520: loss 1.7401758432388306\n",
            "step 4530: loss 1.6530888080596924\n",
            "step 4540: loss 1.6446824073791504\n",
            "step 4550: loss 1.7540028095245361\n",
            "step 4560: loss 1.6215578317642212\n",
            "step 4570: loss 1.6513835191726685\n",
            "step 4580: loss 1.7153559923171997\n",
            "step 4590: loss 1.6328123807907104\n",
            "step 4600: train loss 1.6883, val loss 1.6941\n",
            "step 4600: loss 1.5694613456726074\n",
            "step 4610: loss 1.703223705291748\n",
            "step 4620: loss 1.7853577136993408\n",
            "step 4630: loss 1.7541468143463135\n",
            "step 4640: loss 1.8144742250442505\n",
            "step 4650: loss 1.7076640129089355\n",
            "step 4660: loss 1.669792652130127\n",
            "step 4670: loss 1.6837059259414673\n",
            "step 4680: loss 1.7474762201309204\n",
            "step 4690: loss 1.6498483419418335\n",
            "step 4700: train loss 1.6737, val loss 1.6877\n",
            "step 4700: loss 1.6452709436416626\n",
            "step 4710: loss 1.7163251638412476\n",
            "step 4720: loss 1.5972537994384766\n",
            "step 4730: loss 1.5946862697601318\n",
            "step 4740: loss 1.8046528100967407\n",
            "step 4750: loss 1.7222872972488403\n",
            "step 4760: loss 1.7385926246643066\n",
            "step 4770: loss 1.8047071695327759\n",
            "step 4780: loss 1.7188303470611572\n",
            "step 4790: loss 1.6921623945236206\n",
            "step 4800: train loss 1.6666, val loss 1.6808\n",
            "step 4800: loss 1.71416437625885\n",
            "step 4810: loss 1.7677383422851562\n",
            "step 4820: loss 1.6159251928329468\n",
            "step 4830: loss 1.6522005796432495\n",
            "step 4840: loss 1.7669702768325806\n",
            "step 4850: loss 1.6054219007492065\n",
            "step 4860: loss 1.636268973350525\n",
            "step 4870: loss 1.699548363685608\n",
            "step 4880: loss 1.6293292045593262\n",
            "step 4890: loss 1.6400015354156494\n",
            "step 4900: train loss 1.6714, val loss 1.6806\n",
            "step 4900: loss 1.74204683303833\n",
            "step 4910: loss 1.6747314929962158\n",
            "step 4920: loss 1.6884348392486572\n",
            "step 4930: loss 1.7829633951187134\n",
            "step 4940: loss 1.634525179862976\n",
            "step 4950: loss 1.693172574043274\n",
            "step 4960: loss 1.6669150590896606\n",
            "step 4970: loss 1.6219813823699951\n",
            "step 4980: loss 1.8207165002822876\n",
            "step 4990: loss 1.628682255744934\n",
            "step 4999: train loss 1.6628, val loss 1.6754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pth_gpu = '/content/checkpoint/checkpoint_epoch-4999_26.10.2023_09:46:29.pt'\n",
        "model.load_state_dict(torch.load(model_pth_gpu))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM8-IEvHYcBj",
        "outputId": "51828e91-ddec-4c9a-f56f-f03fdb9eaf0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 30_000\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6cEyX2hYfd-",
        "outputId": "9e7b64b9-a156-4717-ff9d-94f49984befc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 1.6645, val loss 1.6738\n",
            "step 100: train loss 1.6627, val loss 1.6819\n",
            "step 200: train loss 1.6601, val loss 1.6726\n",
            "step 300: train loss 1.6416, val loss 1.6537\n",
            "step 400: train loss 1.6432, val loss 1.6573\n",
            "step 500: train loss 1.6391, val loss 1.6465\n",
            "step 600: train loss 1.6462, val loss 1.6514\n",
            "step 700: train loss 1.6368, val loss 1.6470\n",
            "step 800: train loss 1.6303, val loss 1.6368\n",
            "step 900: train loss 1.6278, val loss 1.6445\n",
            "step 1000: train loss 1.6276, val loss 1.6434\n",
            "step 1100: train loss 1.6218, val loss 1.6277\n",
            "step 1200: train loss 1.6143, val loss 1.6182\n",
            "step 1300: train loss 1.6069, val loss 1.6207\n",
            "step 1400: train loss 1.6152, val loss 1.6265\n",
            "step 1500: train loss 1.5945, val loss 1.6202\n",
            "step 1600: train loss 1.6015, val loss 1.6250\n",
            "step 1700: train loss 1.6034, val loss 1.6130\n",
            "step 1800: train loss 1.5975, val loss 1.6183\n",
            "step 1900: train loss 1.6032, val loss 1.6110\n",
            "step 2000: train loss 1.6026, val loss 1.6178\n",
            "step 2100: train loss 1.5995, val loss 1.6072\n",
            "step 2200: train loss 1.5899, val loss 1.5932\n",
            "step 2300: train loss 1.5840, val loss 1.6054\n",
            "step 2400: train loss 1.5876, val loss 1.6016\n",
            "step 2500: train loss 1.5745, val loss 1.6009\n",
            "step 2600: train loss 1.5800, val loss 1.6066\n",
            "step 2700: train loss 1.5774, val loss 1.5934\n",
            "step 2800: train loss 1.5785, val loss 1.6098\n",
            "step 2900: train loss 1.5787, val loss 1.5909\n",
            "step 3000: train loss 1.5794, val loss 1.5923\n",
            "step 3100: train loss 1.5663, val loss 1.5949\n",
            "step 3200: train loss 1.5623, val loss 1.5858\n",
            "step 3300: train loss 1.5557, val loss 1.5929\n",
            "step 3400: train loss 1.5643, val loss 1.5845\n",
            "step 3500: train loss 1.5670, val loss 1.5802\n",
            "step 3600: train loss 1.5458, val loss 1.5817\n",
            "step 3700: train loss 1.5621, val loss 1.5774\n",
            "step 3800: train loss 1.5549, val loss 1.5736\n",
            "step 3900: train loss 1.5588, val loss 1.5823\n",
            "step 4000: train loss 1.5546, val loss 1.5752\n",
            "step 4100: train loss 1.5569, val loss 1.5682\n",
            "step 4200: train loss 1.5531, val loss 1.5798\n",
            "step 4300: train loss 1.5451, val loss 1.5594\n",
            "step 4400: train loss 1.5513, val loss 1.5753\n",
            "step 4500: train loss 1.5491, val loss 1.5714\n",
            "step 4600: train loss 1.5411, val loss 1.5677\n",
            "step 4700: train loss 1.5412, val loss 1.5709\n",
            "step 4800: train loss 1.5353, val loss 1.5548\n",
            "step 4900: train loss 1.5440, val loss 1.5569\n",
            "step 5000: train loss 1.5421, val loss 1.5594\n",
            "step 5100: train loss 1.5289, val loss 1.5509\n",
            "step 5200: train loss 1.5383, val loss 1.5634\n",
            "step 5300: train loss 1.5241, val loss 1.5509\n",
            "step 5400: train loss 1.5306, val loss 1.5587\n",
            "step 5500: train loss 1.5379, val loss 1.5487\n",
            "step 5600: train loss 1.5268, val loss 1.5487\n",
            "step 5700: train loss 1.5328, val loss 1.5522\n",
            "step 5800: train loss 1.5266, val loss 1.5550\n",
            "step 5900: train loss 1.5212, val loss 1.5502\n",
            "step 6000: train loss 1.5092, val loss 1.5338\n",
            "step 6100: train loss 1.5158, val loss 1.5508\n",
            "step 6200: train loss 1.5222, val loss 1.5405\n",
            "step 6300: train loss 1.5262, val loss 1.5568\n",
            "step 6400: train loss 1.5213, val loss 1.5483\n",
            "step 6500: train loss 1.5246, val loss 1.5500\n",
            "step 6600: train loss 1.5128, val loss 1.5251\n",
            "step 6700: train loss 1.5177, val loss 1.5476\n",
            "step 6800: train loss 1.5106, val loss 1.5453\n",
            "step 6900: train loss 1.5077, val loss 1.5417\n",
            "step 7000: train loss 1.5105, val loss 1.5275\n",
            "step 7100: train loss 1.5021, val loss 1.5323\n",
            "step 7200: train loss 1.5061, val loss 1.5316\n",
            "step 7300: train loss 1.5105, val loss 1.5402\n",
            "step 7400: train loss 1.4975, val loss 1.5288\n",
            "step 7500: train loss 1.5016, val loss 1.5371\n",
            "step 7600: train loss 1.5002, val loss 1.5114\n",
            "step 7700: train loss 1.5095, val loss 1.5292\n",
            "step 7800: train loss 1.5140, val loss 1.5397\n",
            "step 7900: train loss 1.4969, val loss 1.5407\n",
            "step 8000: train loss 1.5027, val loss 1.5267\n",
            "step 8100: train loss 1.5031, val loss 1.5248\n",
            "step 8200: train loss 1.4927, val loss 1.5255\n",
            "step 8300: train loss 1.4991, val loss 1.5197\n",
            "step 8400: train loss 1.5030, val loss 1.5328\n",
            "step 8500: train loss 1.4977, val loss 1.5290\n",
            "step 8600: train loss 1.4911, val loss 1.5257\n",
            "step 8700: train loss 1.4952, val loss 1.5216\n",
            "step 8800: train loss 1.4997, val loss 1.5459\n",
            "step 8900: train loss 1.4914, val loss 1.5304\n",
            "step 9000: train loss 1.4958, val loss 1.5390\n",
            "step 9100: train loss 1.4970, val loss 1.5292\n",
            "step 9200: train loss 1.4943, val loss 1.5285\n",
            "step 9300: train loss 1.4851, val loss 1.5119\n",
            "step 9400: train loss 1.4895, val loss 1.5223\n",
            "step 9500: train loss 1.4877, val loss 1.5085\n",
            "step 9600: train loss 1.4888, val loss 1.5171\n",
            "step 9700: train loss 1.4809, val loss 1.5081\n",
            "step 9800: train loss 1.4808, val loss 1.5108\n",
            "step 9900: train loss 1.4854, val loss 1.5252\n",
            "step 10000: train loss 1.4865, val loss 1.5111\n",
            "step 10100: train loss 1.4810, val loss 1.5180\n",
            "step 10200: train loss 1.4756, val loss 1.4978\n",
            "step 10300: train loss 1.4843, val loss 1.5081\n",
            "step 10400: train loss 1.4769, val loss 1.5145\n",
            "step 10500: train loss 1.4819, val loss 1.5090\n",
            "step 10600: train loss 1.4795, val loss 1.5148\n",
            "step 10700: train loss 1.4806, val loss 1.5000\n",
            "step 10800: train loss 1.4737, val loss 1.5119\n",
            "step 10900: train loss 1.4724, val loss 1.5080\n",
            "step 11000: train loss 1.4788, val loss 1.5150\n",
            "step 11100: train loss 1.4807, val loss 1.5005\n",
            "step 11200: train loss 1.4785, val loss 1.5093\n",
            "step 11300: train loss 1.4695, val loss 1.5128\n",
            "step 11400: train loss 1.4764, val loss 1.5103\n",
            "step 11500: train loss 1.4709, val loss 1.4974\n",
            "step 11600: train loss 1.4727, val loss 1.5061\n",
            "step 11700: train loss 1.4643, val loss 1.4982\n",
            "step 11800: train loss 1.4626, val loss 1.5033\n",
            "step 11900: train loss 1.4756, val loss 1.5162\n",
            "step 12000: train loss 1.4747, val loss 1.5132\n",
            "step 12100: train loss 1.4604, val loss 1.4958\n",
            "step 12200: train loss 1.4640, val loss 1.5080\n",
            "step 12300: train loss 1.4729, val loss 1.4901\n",
            "step 12400: train loss 1.4630, val loss 1.5071\n",
            "step 12500: train loss 1.4649, val loss 1.4998\n",
            "step 12600: train loss 1.4720, val loss 1.5012\n",
            "step 12700: train loss 1.4536, val loss 1.5008\n",
            "step 12800: train loss 1.4699, val loss 1.5059\n",
            "step 12900: train loss 1.4552, val loss 1.4969\n",
            "step 13000: train loss 1.4570, val loss 1.5053\n",
            "step 13100: train loss 1.4644, val loss 1.5007\n",
            "step 13200: train loss 1.4680, val loss 1.5073\n",
            "step 13300: train loss 1.4626, val loss 1.5063\n",
            "step 13400: train loss 1.4598, val loss 1.4892\n",
            "step 13500: train loss 1.4596, val loss 1.4942\n",
            "step 13600: train loss 1.4508, val loss 1.4942\n",
            "step 13700: train loss 1.4588, val loss 1.4980\n",
            "step 13800: train loss 1.4601, val loss 1.4932\n",
            "step 13900: train loss 1.4623, val loss 1.5145\n",
            "step 14000: train loss 1.4596, val loss 1.4986\n",
            "step 14100: train loss 1.4558, val loss 1.4959\n",
            "step 14200: train loss 1.4540, val loss 1.4823\n",
            "step 14300: train loss 1.4492, val loss 1.4885\n",
            "step 14400: train loss 1.4636, val loss 1.4939\n",
            "step 14500: train loss 1.4559, val loss 1.4916\n",
            "step 14600: train loss 1.4503, val loss 1.4903\n",
            "step 14700: train loss 1.4586, val loss 1.4958\n",
            "step 14800: train loss 1.4483, val loss 1.4945\n",
            "step 14900: train loss 1.4444, val loss 1.4912\n",
            "step 15000: train loss 1.4498, val loss 1.5017\n",
            "step 15100: train loss 1.4466, val loss 1.4965\n",
            "step 15200: train loss 1.4357, val loss 1.4823\n",
            "step 15300: train loss 1.4475, val loss 1.4871\n",
            "step 15400: train loss 1.4535, val loss 1.4918\n",
            "step 15500: train loss 1.4374, val loss 1.4878\n",
            "step 15600: train loss 1.4457, val loss 1.4794\n",
            "step 15700: train loss 1.4561, val loss 1.4950\n",
            "step 15800: train loss 1.4476, val loss 1.4968\n",
            "step 15900: train loss 1.4458, val loss 1.4815\n",
            "step 16000: train loss 1.4491, val loss 1.4896\n",
            "step 16100: train loss 1.4464, val loss 1.4864\n",
            "step 16200: train loss 1.4427, val loss 1.4830\n",
            "step 16300: train loss 1.4413, val loss 1.4945\n",
            "step 16400: train loss 1.4458, val loss 1.4786\n",
            "step 16500: train loss 1.4442, val loss 1.4890\n",
            "step 16600: train loss 1.4412, val loss 1.4893\n",
            "step 16700: train loss 1.4442, val loss 1.4708\n",
            "step 16800: train loss 1.4315, val loss 1.4789\n",
            "step 16900: train loss 1.4438, val loss 1.4817\n",
            "step 17000: train loss 1.4458, val loss 1.4808\n",
            "step 17100: train loss 1.4397, val loss 1.4772\n",
            "step 17200: train loss 1.4365, val loss 1.4829\n",
            "step 17300: train loss 1.4313, val loss 1.4800\n",
            "step 17400: train loss 1.4523, val loss 1.4898\n",
            "step 17500: train loss 1.4280, val loss 1.4818\n",
            "step 17600: train loss 1.4428, val loss 1.4863\n",
            "step 17700: train loss 1.4363, val loss 1.4737\n",
            "step 17800: train loss 1.4432, val loss 1.5023\n",
            "step 17900: train loss 1.4392, val loss 1.4870\n",
            "step 18000: train loss 1.4296, val loss 1.4700\n",
            "step 18100: train loss 1.4337, val loss 1.4782\n",
            "step 18200: train loss 1.4378, val loss 1.4733\n",
            "step 18300: train loss 1.4293, val loss 1.4710\n",
            "step 18400: train loss 1.4309, val loss 1.4729\n",
            "step 18500: train loss 1.4398, val loss 1.4856\n",
            "step 18600: train loss 1.4445, val loss 1.4781\n",
            "step 18700: train loss 1.4274, val loss 1.4753\n",
            "step 18800: train loss 1.4339, val loss 1.4817\n",
            "step 18900: train loss 1.4365, val loss 1.4735\n",
            "step 19000: train loss 1.4340, val loss 1.4726\n",
            "step 19100: train loss 1.4284, val loss 1.4625\n",
            "step 19200: train loss 1.4242, val loss 1.4775\n",
            "step 19300: train loss 1.4255, val loss 1.4805\n",
            "step 19400: train loss 1.4257, val loss 1.4704\n",
            "step 19500: train loss 1.4318, val loss 1.4700\n",
            "step 19600: train loss 1.4329, val loss 1.4800\n",
            "step 19700: train loss 1.4177, val loss 1.4603\n",
            "step 19800: train loss 1.4272, val loss 1.4717\n",
            "step 19900: train loss 1.4264, val loss 1.4717\n",
            "step 20000: train loss 1.4270, val loss 1.4741\n",
            "step 20100: train loss 1.4191, val loss 1.4735\n",
            "step 20200: train loss 1.4279, val loss 1.4721\n",
            "step 20300: train loss 1.4307, val loss 1.4835\n",
            "step 20400: train loss 1.4323, val loss 1.4665\n",
            "step 20500: train loss 1.4380, val loss 1.4802\n",
            "step 20600: train loss 1.4311, val loss 1.4742\n",
            "step 20700: train loss 1.4198, val loss 1.4771\n",
            "step 20800: train loss 1.4277, val loss 1.4644\n",
            "step 20900: train loss 1.4193, val loss 1.4655\n",
            "step 21000: train loss 1.4194, val loss 1.4660\n",
            "step 21100: train loss 1.4250, val loss 1.4681\n",
            "step 21200: train loss 1.4308, val loss 1.4750\n",
            "step 21300: train loss 1.4261, val loss 1.4676\n",
            "step 21400: train loss 1.4284, val loss 1.4619\n",
            "step 21500: train loss 1.4208, val loss 1.4550\n",
            "step 21600: train loss 1.4283, val loss 1.4734\n",
            "step 21700: train loss 1.4177, val loss 1.4610\n",
            "step 21800: train loss 1.4156, val loss 1.4630\n",
            "step 21900: train loss 1.4162, val loss 1.4690\n",
            "step 22000: train loss 1.4257, val loss 1.4593\n",
            "step 22100: train loss 1.4204, val loss 1.4761\n",
            "step 22200: train loss 1.4211, val loss 1.4724\n",
            "step 22300: train loss 1.4251, val loss 1.4676\n",
            "step 22400: train loss 1.4313, val loss 1.4650\n",
            "step 22500: train loss 1.4209, val loss 1.4766\n",
            "step 22600: train loss 1.4210, val loss 1.4650\n",
            "step 22700: train loss 1.4109, val loss 1.4676\n",
            "step 22800: train loss 1.4211, val loss 1.4619\n",
            "step 22900: train loss 1.4266, val loss 1.4605\n",
            "step 23000: train loss 1.4158, val loss 1.4721\n",
            "step 23100: train loss 1.4137, val loss 1.4631\n",
            "step 23200: train loss 1.4135, val loss 1.4604\n",
            "step 23300: train loss 1.4080, val loss 1.4645\n",
            "step 23400: train loss 1.4238, val loss 1.4656\n",
            "step 23500: train loss 1.4142, val loss 1.4657\n",
            "step 23600: train loss 1.4077, val loss 1.4570\n",
            "step 23700: train loss 1.4013, val loss 1.4578\n",
            "step 23800: train loss 1.4142, val loss 1.4681\n",
            "step 23900: train loss 1.4183, val loss 1.4599\n",
            "step 24000: train loss 1.4138, val loss 1.4574\n",
            "step 24100: train loss 1.4173, val loss 1.4651\n",
            "step 24200: train loss 1.4235, val loss 1.4747\n",
            "step 24300: train loss 1.4111, val loss 1.4506\n",
            "step 24400: train loss 1.4219, val loss 1.4648\n",
            "step 24500: train loss 1.4059, val loss 1.4546\n",
            "step 24600: train loss 1.4119, val loss 1.4825\n",
            "step 24700: train loss 1.4025, val loss 1.4604\n",
            "step 24800: train loss 1.4172, val loss 1.4650\n",
            "step 24900: train loss 1.4108, val loss 1.4688\n",
            "step 25000: train loss 1.4152, val loss 1.4689\n",
            "step 25100: train loss 1.4116, val loss 1.4614\n",
            "step 25200: train loss 1.3959, val loss 1.4546\n",
            "step 25300: train loss 1.4056, val loss 1.4624\n",
            "step 25400: train loss 1.4250, val loss 1.4605\n",
            "step 25500: train loss 1.4124, val loss 1.4629\n",
            "step 25600: train loss 1.4099, val loss 1.4608\n",
            "step 25700: train loss 1.4210, val loss 1.4660\n",
            "step 25800: train loss 1.4056, val loss 1.4752\n",
            "step 25900: train loss 1.4122, val loss 1.4601\n",
            "step 26000: train loss 1.4049, val loss 1.4622\n",
            "step 26100: train loss 1.4100, val loss 1.4686\n",
            "step 26200: train loss 1.4071, val loss 1.4732\n",
            "step 26300: train loss 1.4060, val loss 1.4603\n",
            "step 26400: train loss 1.4064, val loss 1.4528\n",
            "step 26500: train loss 1.4153, val loss 1.4706\n",
            "step 26600: train loss 1.4073, val loss 1.4540\n",
            "step 26700: train loss 1.4074, val loss 1.4641\n",
            "step 26800: train loss 1.4131, val loss 1.4544\n",
            "step 26900: train loss 1.3999, val loss 1.4614\n",
            "step 27000: train loss 1.3948, val loss 1.4551\n",
            "step 27100: train loss 1.4119, val loss 1.4448\n",
            "step 27200: train loss 1.4022, val loss 1.4663\n",
            "step 27300: train loss 1.4068, val loss 1.4669\n",
            "step 27400: train loss 1.4032, val loss 1.4668\n",
            "step 27500: train loss 1.4029, val loss 1.4606\n",
            "step 27600: train loss 1.3989, val loss 1.4531\n",
            "step 27700: train loss 1.4057, val loss 1.4516\n",
            "step 27800: train loss 1.4050, val loss 1.4567\n",
            "step 27900: train loss 1.4049, val loss 1.4524\n",
            "step 28000: train loss 1.4055, val loss 1.4555\n",
            "step 28100: train loss 1.3996, val loss 1.4465\n",
            "step 28200: train loss 1.4024, val loss 1.4659\n",
            "step 28300: train loss 1.4024, val loss 1.4570\n",
            "step 28400: train loss 1.3972, val loss 1.4611\n",
            "step 28500: train loss 1.4075, val loss 1.4621\n",
            "step 28600: train loss 1.3950, val loss 1.4593\n",
            "step 28700: train loss 1.3955, val loss 1.4598\n",
            "step 28800: train loss 1.3975, val loss 1.4619\n",
            "step 28900: train loss 1.4001, val loss 1.4693\n",
            "step 29000: train loss 1.4045, val loss 1.4522\n",
            "step 29100: train loss 1.4083, val loss 1.4638\n",
            "step 29200: train loss 1.3893, val loss 1.4608\n",
            "step 29300: train loss 1.3989, val loss 1.4529\n",
            "step 29400: train loss 1.3923, val loss 1.4582\n",
            "step 29500: train loss 1.3893, val loss 1.4596\n",
            "step 29600: train loss 1.4041, val loss 1.4579\n",
            "step 29700: train loss 1.4000, val loss 1.4718\n",
            "step 29800: train loss 1.3998, val loss 1.4621\n",
            "step 29900: train loss 1.4033, val loss 1.4626\n",
            "step 29999: train loss 1.3902, val loss 1.4553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKRUbboz6Xva",
        "outputId": "bba3235f-2159-40d0-d499-49eb18f6a536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "a-place, \"to kind on my next had a strong thing agreement  practic injoy deale by his business, there was a former aband with some of Ecush for a\n",
            "macan of three new many wravek\n",
            "rough\n",
            "builts, were studing gathering freedom, always told Runivires of such did not one or eaching contributing ‘said, my telling other pa-\n",
            "\n",
            "tial complaintently isn't's issue demonstance. No agagnity and lessonal moon. Before though criminated to meditations (the end formal government sent as\n",
            "a year coal. Inconcequent fixed at his 20earing plans passaged one tirls had been seated wown found and magnity had to people in each other the bid, whicked he\n",
            "indidence perfically, with committee with rise by Kemilitars’ Persian Cabinet and most beffited the owned root Veedoching Housing Amerous, worse yarn and bongue ward. Before the follow of ignorance to her further Tances investigation is institutions orly extended me abording 1s a unlike Dhirubhai’s faceful.\n",
            "Whesto‘leant has no Sardar Par and there and head, using out that would. I firest in seems of manageers. It office and the secustary were their had, Natai, the good of mid others raws in than the government resturing for\n",
            "timely of the\n",
            "hands. Reminiscences often mind is\n",
            "tatte ministeres was society sported in feeds. By\n",
            "cock selected mankan gets poulare pibate,\n",
            "Singh’s curties further thesere. Wadia politically babad. Prahesion were taution rurously compl. They were isperce propoundended in\n",
            "mind:\n",
            "10 173 the bodies in 194filite of Grace. Devi after pressf consulten insported to the perpet dinner. Seuths. Nehru and\n",
            "manager friend of my important at the instretimes and the kept in the additional pressure was a rivals finals find this subsidity distagockminissments\n",
            "were interest as a resenctive stoppanied sons on 32 F annound Porturely being in the UP‘CIC\n",
            "\n",
            " 1955\n",
            "attackent out of the word down.\" I earings hidden seeils climb for a fresh both teach of perfective figure ting a few Director of India wasn't parliament\n",
            "than a visit. The bond. The huge work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pth_gpu = '/content/checkpoint/checkpoint_epoch-29999_26.10.2023_10:14:54.pt'\n",
        "model.load_state_dict(torch.load(model_pth_gpu))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYQELpGwexQB",
        "outputId": "3f8ee775-dd13-4fee-a7ff-2b405012269a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev_iter = 30_000\n",
        "max_iters = 60_000\n",
        "for iter in range(prev_iter, max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2rV66cXe67X",
        "outputId": "f1d1fb34-6b1d-4045-b3d0-90f23b551105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 30000: train loss 1.3955, val loss 1.4571\n",
            "step 30100: train loss 1.4028, val loss 1.4594\n",
            "step 30200: train loss 1.3990, val loss 1.4595\n",
            "step 30300: train loss 1.3976, val loss 1.4611\n",
            "step 30400: train loss 1.3924, val loss 1.4520\n",
            "step 30500: train loss 1.3955, val loss 1.4473\n",
            "step 30600: train loss 1.3989, val loss 1.4579\n",
            "step 30700: train loss 1.3985, val loss 1.4532\n",
            "step 30800: train loss 1.3867, val loss 1.4505\n",
            "step 30900: train loss 1.4000, val loss 1.4482\n",
            "step 31000: train loss 1.3893, val loss 1.4548\n",
            "step 31100: train loss 1.3979, val loss 1.4534\n",
            "step 31200: train loss 1.3983, val loss 1.4689\n",
            "step 31300: train loss 1.3879, val loss 1.4537\n",
            "step 31400: train loss 1.3929, val loss 1.4522\n",
            "step 31500: train loss 1.3996, val loss 1.4502\n",
            "step 31600: train loss 1.3936, val loss 1.4541\n",
            "step 31700: train loss 1.3906, val loss 1.4584\n",
            "step 31800: train loss 1.3944, val loss 1.4477\n",
            "step 31900: train loss 1.3980, val loss 1.4532\n",
            "step 32000: train loss 1.3963, val loss 1.4580\n",
            "step 32100: train loss 1.3926, val loss 1.4547\n",
            "step 32200: train loss 1.3911, val loss 1.4445\n",
            "step 32300: train loss 1.3916, val loss 1.4543\n",
            "step 32400: train loss 1.3821, val loss 1.4400\n",
            "step 32500: train loss 1.3928, val loss 1.4411\n",
            "step 32600: train loss 1.3920, val loss 1.4438\n",
            "step 32700: train loss 1.3921, val loss 1.4420\n",
            "step 32800: train loss 1.3940, val loss 1.4413\n",
            "step 32900: train loss 1.4028, val loss 1.4543\n",
            "step 33000: train loss 1.3927, val loss 1.4415\n",
            "step 33100: train loss 1.3859, val loss 1.4485\n",
            "step 33200: train loss 1.4007, val loss 1.4426\n",
            "step 33300: train loss 1.3944, val loss 1.4347\n",
            "step 33400: train loss 1.3883, val loss 1.4582\n",
            "step 33500: train loss 1.3908, val loss 1.4504\n",
            "step 33600: train loss 1.3906, val loss 1.4404\n",
            "step 33700: train loss 1.3909, val loss 1.4465\n",
            "step 33800: train loss 1.3925, val loss 1.4468\n",
            "step 33900: train loss 1.3834, val loss 1.4382\n",
            "step 34000: train loss 1.3943, val loss 1.4451\n",
            "step 34100: train loss 1.3895, val loss 1.4468\n",
            "step 34200: train loss 1.3868, val loss 1.4388\n",
            "step 34300: train loss 1.3931, val loss 1.4524\n",
            "step 34400: train loss 1.3933, val loss 1.4447\n",
            "step 34500: train loss 1.3881, val loss 1.4338\n",
            "step 34600: train loss 1.3941, val loss 1.4519\n",
            "step 34700: train loss 1.3837, val loss 1.4399\n",
            "step 34800: train loss 1.3914, val loss 1.4439\n",
            "step 34900: train loss 1.3897, val loss 1.4350\n",
            "step 35000: train loss 1.3914, val loss 1.4563\n",
            "step 35100: train loss 1.3896, val loss 1.4362\n",
            "step 35200: train loss 1.3829, val loss 1.4408\n",
            "step 35300: train loss 1.3902, val loss 1.4403\n",
            "step 35400: train loss 1.3900, val loss 1.4571\n",
            "step 35500: train loss 1.3815, val loss 1.4529\n",
            "step 35600: train loss 1.3860, val loss 1.4470\n",
            "step 35700: train loss 1.3901, val loss 1.4463\n",
            "step 35800: train loss 1.3860, val loss 1.4369\n",
            "step 35900: train loss 1.3830, val loss 1.4324\n",
            "step 36000: train loss 1.3779, val loss 1.4400\n",
            "step 36100: train loss 1.3872, val loss 1.4399\n",
            "step 36200: train loss 1.3848, val loss 1.4453\n",
            "step 36300: train loss 1.3839, val loss 1.4495\n",
            "step 36400: train loss 1.3802, val loss 1.4466\n",
            "step 36500: train loss 1.3850, val loss 1.4389\n",
            "step 36600: train loss 1.3799, val loss 1.4463\n",
            "step 36700: train loss 1.3829, val loss 1.4543\n",
            "step 36800: train loss 1.3904, val loss 1.4473\n",
            "step 36900: train loss 1.3775, val loss 1.4484\n",
            "step 37000: train loss 1.3875, val loss 1.4446\n",
            "step 37100: train loss 1.3891, val loss 1.4448\n",
            "step 37200: train loss 1.3774, val loss 1.4444\n",
            "step 37300: train loss 1.3740, val loss 1.4408\n",
            "step 37400: train loss 1.3885, val loss 1.4373\n",
            "step 37500: train loss 1.3760, val loss 1.4483\n",
            "step 37600: train loss 1.3796, val loss 1.4419\n",
            "step 37700: train loss 1.3799, val loss 1.4348\n",
            "step 37800: train loss 1.3776, val loss 1.4396\n",
            "step 37900: train loss 1.3763, val loss 1.4409\n",
            "step 38000: train loss 1.3790, val loss 1.4321\n",
            "step 38100: train loss 1.3839, val loss 1.4405\n",
            "step 38200: train loss 1.3830, val loss 1.4510\n",
            "step 38300: train loss 1.3877, val loss 1.4382\n",
            "step 38400: train loss 1.3816, val loss 1.4535\n",
            "step 38500: train loss 1.3820, val loss 1.4540\n",
            "step 38600: train loss 1.3830, val loss 1.4388\n",
            "step 38700: train loss 1.3811, val loss 1.4531\n",
            "step 38800: train loss 1.3867, val loss 1.4434\n",
            "step 38900: train loss 1.3762, val loss 1.4507\n",
            "step 39000: train loss 1.3735, val loss 1.4445\n",
            "step 39100: train loss 1.3831, val loss 1.4496\n",
            "step 39200: train loss 1.3711, val loss 1.4337\n",
            "step 39300: train loss 1.3756, val loss 1.4444\n",
            "step 39400: train loss 1.3779, val loss 1.4527\n",
            "step 39500: train loss 1.3801, val loss 1.4414\n",
            "step 39600: train loss 1.3757, val loss 1.4298\n",
            "step 39700: train loss 1.3736, val loss 1.4465\n",
            "step 39800: train loss 1.3723, val loss 1.4328\n",
            "step 39900: train loss 1.3829, val loss 1.4320\n",
            "step 40000: train loss 1.3777, val loss 1.4320\n",
            "step 40100: train loss 1.3829, val loss 1.4422\n",
            "step 40200: train loss 1.3852, val loss 1.4365\n",
            "step 40300: train loss 1.3803, val loss 1.4345\n",
            "step 40400: train loss 1.3783, val loss 1.4444\n",
            "step 40500: train loss 1.3736, val loss 1.4278\n",
            "step 40600: train loss 1.3692, val loss 1.4460\n",
            "step 40700: train loss 1.3736, val loss 1.4350\n",
            "step 40800: train loss 1.3741, val loss 1.4495\n",
            "step 40900: train loss 1.3813, val loss 1.4419\n",
            "step 41000: train loss 1.3738, val loss 1.4424\n",
            "step 41100: train loss 1.3780, val loss 1.4445\n",
            "step 41200: train loss 1.3782, val loss 1.4358\n",
            "step 41300: train loss 1.3717, val loss 1.4363\n",
            "step 41400: train loss 1.3820, val loss 1.4385\n",
            "step 41500: train loss 1.3805, val loss 1.4328\n",
            "step 41600: train loss 1.3660, val loss 1.4355\n",
            "step 41700: train loss 1.3699, val loss 1.4449\n",
            "step 41800: train loss 1.3749, val loss 1.4415\n",
            "step 41900: train loss 1.3756, val loss 1.4395\n",
            "step 42000: train loss 1.3805, val loss 1.4313\n",
            "step 42100: train loss 1.3726, val loss 1.4432\n",
            "step 42200: train loss 1.3653, val loss 1.4359\n",
            "step 42300: train loss 1.3702, val loss 1.4367\n",
            "step 42400: train loss 1.3754, val loss 1.4369\n",
            "step 42500: train loss 1.3769, val loss 1.4434\n",
            "step 42600: train loss 1.3773, val loss 1.4469\n",
            "step 42700: train loss 1.3740, val loss 1.4403\n",
            "step 42800: train loss 1.3712, val loss 1.4350\n",
            "step 42900: train loss 1.3672, val loss 1.4271\n",
            "step 43000: train loss 1.3735, val loss 1.4295\n",
            "step 43100: train loss 1.3650, val loss 1.4341\n",
            "step 43200: train loss 1.3789, val loss 1.4542\n",
            "step 43300: train loss 1.3762, val loss 1.4257\n",
            "step 43400: train loss 1.3818, val loss 1.4400\n",
            "step 43500: train loss 1.3669, val loss 1.4329\n",
            "step 43600: train loss 1.3673, val loss 1.4307\n",
            "step 43700: train loss 1.3644, val loss 1.4305\n",
            "step 43800: train loss 1.3714, val loss 1.4408\n",
            "step 43900: train loss 1.3747, val loss 1.4402\n",
            "step 44000: train loss 1.3725, val loss 1.4370\n",
            "step 44100: train loss 1.3784, val loss 1.4298\n",
            "step 44200: train loss 1.3665, val loss 1.4281\n",
            "step 44300: train loss 1.3705, val loss 1.4408\n",
            "step 44400: train loss 1.3669, val loss 1.4351\n",
            "step 44500: train loss 1.3798, val loss 1.4411\n",
            "step 44600: train loss 1.3647, val loss 1.4386\n",
            "step 44700: train loss 1.3703, val loss 1.4454\n",
            "step 44800: train loss 1.3698, val loss 1.4269\n",
            "step 44900: train loss 1.3697, val loss 1.4335\n",
            "step 45000: train loss 1.3750, val loss 1.4489\n",
            "step 45100: train loss 1.3788, val loss 1.4361\n",
            "step 45200: train loss 1.3716, val loss 1.4277\n",
            "step 45300: train loss 1.3718, val loss 1.4322\n",
            "step 45400: train loss 1.3715, val loss 1.4392\n",
            "step 45500: train loss 1.3761, val loss 1.4264\n",
            "step 45600: train loss 1.3771, val loss 1.4323\n",
            "step 45700: train loss 1.3673, val loss 1.4317\n",
            "step 45800: train loss 1.3723, val loss 1.4421\n",
            "step 45900: train loss 1.3685, val loss 1.4394\n",
            "step 46000: train loss 1.3691, val loss 1.4279\n",
            "step 46100: train loss 1.3676, val loss 1.4228\n",
            "step 46200: train loss 1.3691, val loss 1.4390\n",
            "step 46300: train loss 1.3706, val loss 1.4334\n",
            "step 46400: train loss 1.3707, val loss 1.4483\n",
            "step 46500: train loss 1.3665, val loss 1.4344\n",
            "step 46600: train loss 1.3711, val loss 1.4341\n",
            "step 46700: train loss 1.3654, val loss 1.4402\n",
            "step 46800: train loss 1.3696, val loss 1.4224\n",
            "step 46900: train loss 1.3642, val loss 1.4157\n",
            "step 47000: train loss 1.3699, val loss 1.4315\n",
            "step 47100: train loss 1.3782, val loss 1.4379\n",
            "step 47200: train loss 1.3698, val loss 1.4419\n",
            "step 47300: train loss 1.3646, val loss 1.4459\n",
            "step 47400: train loss 1.3698, val loss 1.4260\n",
            "step 47500: train loss 1.3663, val loss 1.4326\n",
            "step 47600: train loss 1.3687, val loss 1.4419\n",
            "step 47700: train loss 1.3672, val loss 1.4361\n",
            "step 47800: train loss 1.3729, val loss 1.4386\n",
            "step 47900: train loss 1.3682, val loss 1.4435\n",
            "step 48000: train loss 1.3602, val loss 1.4460\n",
            "step 48100: train loss 1.3736, val loss 1.4275\n",
            "step 48200: train loss 1.3757, val loss 1.4291\n",
            "step 48300: train loss 1.3677, val loss 1.4392\n",
            "step 48400: train loss 1.3685, val loss 1.4362\n",
            "step 48500: train loss 1.3693, val loss 1.4209\n",
            "step 48600: train loss 1.3667, val loss 1.4314\n",
            "step 48700: train loss 1.3653, val loss 1.4366\n",
            "step 48800: train loss 1.3802, val loss 1.4315\n",
            "step 48900: train loss 1.3651, val loss 1.4449\n",
            "step 49000: train loss 1.3586, val loss 1.4404\n",
            "step 49100: train loss 1.3594, val loss 1.4359\n",
            "step 49200: train loss 1.3723, val loss 1.4323\n",
            "step 49300: train loss 1.3701, val loss 1.4361\n",
            "step 49400: train loss 1.3622, val loss 1.4352\n",
            "step 49500: train loss 1.3558, val loss 1.4227\n",
            "step 49600: train loss 1.3652, val loss 1.4247\n",
            "step 49700: train loss 1.3619, val loss 1.4357\n",
            "step 49800: train loss 1.3666, val loss 1.4318\n",
            "step 49900: train loss 1.3658, val loss 1.4268\n",
            "step 50000: train loss 1.3694, val loss 1.4392\n",
            "step 50100: train loss 1.3659, val loss 1.4309\n",
            "step 50200: train loss 1.3630, val loss 1.4284\n",
            "step 50300: train loss 1.3710, val loss 1.4333\n",
            "step 50400: train loss 1.3606, val loss 1.4308\n",
            "step 50500: train loss 1.3683, val loss 1.4333\n",
            "step 50600: train loss 1.3559, val loss 1.4259\n",
            "step 50700: train loss 1.3681, val loss 1.4330\n",
            "step 50800: train loss 1.3639, val loss 1.4226\n",
            "step 50900: train loss 1.3634, val loss 1.4299\n",
            "step 51000: train loss 1.3586, val loss 1.4275\n",
            "step 51100: train loss 1.3603, val loss 1.4153\n",
            "step 51200: train loss 1.3622, val loss 1.4146\n",
            "step 51300: train loss 1.3645, val loss 1.4326\n",
            "step 51400: train loss 1.3622, val loss 1.4284\n",
            "step 51500: train loss 1.3574, val loss 1.4276\n",
            "step 51600: train loss 1.3613, val loss 1.4386\n",
            "step 51700: train loss 1.3680, val loss 1.4286\n",
            "step 51800: train loss 1.3657, val loss 1.4301\n",
            "step 51900: train loss 1.3605, val loss 1.4410\n",
            "step 52000: train loss 1.3596, val loss 1.4208\n",
            "step 52100: train loss 1.3624, val loss 1.4304\n",
            "step 52200: train loss 1.3663, val loss 1.4286\n",
            "step 52300: train loss 1.3636, val loss 1.4318\n",
            "step 52400: train loss 1.3719, val loss 1.4475\n",
            "step 52500: train loss 1.3638, val loss 1.4326\n",
            "step 52600: train loss 1.3599, val loss 1.4270\n",
            "step 52700: train loss 1.3635, val loss 1.4260\n",
            "step 52800: train loss 1.3697, val loss 1.4270\n",
            "step 52900: train loss 1.3552, val loss 1.4299\n",
            "step 53000: train loss 1.3626, val loss 1.4259\n",
            "step 53100: train loss 1.3571, val loss 1.4158\n",
            "step 53200: train loss 1.3596, val loss 1.4368\n",
            "step 53300: train loss 1.3678, val loss 1.4409\n",
            "step 53400: train loss 1.3592, val loss 1.4279\n",
            "step 53500: train loss 1.3623, val loss 1.4224\n",
            "step 53600: train loss 1.3612, val loss 1.4243\n",
            "step 53700: train loss 1.3697, val loss 1.4252\n",
            "step 53800: train loss 1.3600, val loss 1.4306\n",
            "step 53900: train loss 1.3576, val loss 1.4276\n",
            "step 54000: train loss 1.3624, val loss 1.4218\n",
            "step 54100: train loss 1.3645, val loss 1.4298\n",
            "step 54200: train loss 1.3630, val loss 1.4328\n",
            "step 54300: train loss 1.3582, val loss 1.4360\n",
            "step 54400: train loss 1.3545, val loss 1.4228\n",
            "step 54500: train loss 1.3558, val loss 1.4348\n",
            "step 54600: train loss 1.3604, val loss 1.4314\n",
            "step 54700: train loss 1.3623, val loss 1.4314\n",
            "step 54800: train loss 1.3623, val loss 1.4200\n",
            "step 54900: train loss 1.3678, val loss 1.4231\n",
            "step 55000: train loss 1.3557, val loss 1.4187\n",
            "step 55100: train loss 1.3538, val loss 1.4228\n",
            "step 55200: train loss 1.3578, val loss 1.4308\n",
            "step 55300: train loss 1.3647, val loss 1.4288\n",
            "step 55400: train loss 1.3594, val loss 1.4300\n",
            "step 55500: train loss 1.3563, val loss 1.4350\n",
            "step 55600: train loss 1.3640, val loss 1.4273\n",
            "step 55700: train loss 1.3628, val loss 1.4367\n",
            "step 55800: train loss 1.3591, val loss 1.4313\n",
            "step 55900: train loss 1.3634, val loss 1.4380\n",
            "step 56000: train loss 1.3592, val loss 1.4356\n",
            "step 56100: train loss 1.3597, val loss 1.4339\n",
            "step 56200: train loss 1.3430, val loss 1.4279\n",
            "step 56300: train loss 1.3593, val loss 1.4315\n",
            "step 56400: train loss 1.3584, val loss 1.4265\n",
            "step 56500: train loss 1.3566, val loss 1.4380\n",
            "step 56600: train loss 1.3500, val loss 1.4352\n",
            "step 56700: train loss 1.3593, val loss 1.4406\n",
            "step 56800: train loss 1.3502, val loss 1.4317\n",
            "step 56900: train loss 1.3576, val loss 1.4286\n",
            "step 57000: train loss 1.3586, val loss 1.4367\n",
            "step 57100: train loss 1.3568, val loss 1.4318\n",
            "step 57200: train loss 1.3618, val loss 1.4234\n",
            "step 57300: train loss 1.3501, val loss 1.4170\n",
            "step 57400: train loss 1.3510, val loss 1.4237\n",
            "step 57500: train loss 1.3610, val loss 1.4283\n",
            "step 57600: train loss 1.3583, val loss 1.4272\n",
            "step 57700: train loss 1.3547, val loss 1.4238\n",
            "step 57800: train loss 1.3552, val loss 1.4393\n",
            "step 57900: train loss 1.3521, val loss 1.4293\n",
            "step 58000: train loss 1.3568, val loss 1.4217\n",
            "step 58100: train loss 1.3489, val loss 1.4309\n",
            "step 58200: train loss 1.3559, val loss 1.4349\n",
            "step 58300: train loss 1.3566, val loss 1.4228\n",
            "step 58400: train loss 1.3599, val loss 1.4307\n",
            "step 58500: train loss 1.3566, val loss 1.4278\n",
            "step 58600: train loss 1.3552, val loss 1.4261\n",
            "step 58700: train loss 1.3649, val loss 1.4214\n",
            "step 58800: train loss 1.3557, val loss 1.4266\n",
            "step 58900: train loss 1.3542, val loss 1.4233\n",
            "step 59000: train loss 1.3583, val loss 1.4381\n",
            "step 59100: train loss 1.3460, val loss 1.4358\n",
            "step 59200: train loss 1.3632, val loss 1.4314\n",
            "step 59300: train loss 1.3432, val loss 1.4355\n",
            "step 59400: train loss 1.3556, val loss 1.4084\n",
            "step 59500: train loss 1.3531, val loss 1.4306\n",
            "step 59600: train loss 1.3526, val loss 1.4246\n",
            "step 59700: train loss 1.3481, val loss 1.4327\n",
            "step 59800: train loss 1.3547, val loss 1.4137\n",
            "step 59900: train loss 1.3596, val loss 1.4273\n",
            "step 59999: train loss 1.3551, val loss 1.4322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNubpc1QfAsu",
        "outputId": "bfcf69c9-749e-49e7-8b51-4a7a13e4b496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dhirubhai, and who was two partnal is reality over dignity in full harded one a\n",
            "serious-\n",
            "tain andagan\n",
            "in winds. At a small and stinguing old wrong with some procoving,\n",
            "proposing to make uniobed stayed uttention to rather tax see show to end out a political internationally hands in firmax, asdored with the Nehru\n",
            "found Emergency's adothing spiritual still wiminds,\n",
            "normally bormable if he was each other personal Bolla\n",
            "to much.\n",
            "Dhirubhai had been decided Nehru directly the love of the essert of emperous through to 4]44 he had basked\n",
            "it. If the promise. This Group Mohamman and Took pundathi. Reliance ham seen tobe matters\n",
            "minds\n",
            "to be later by bailling on a People Gombani, men and is a meal of clear the meeting India and\n",
            "was usewhere no\n",
            "soughernalized\n",
            "on a watvish\n",
            "promisor.\"\n",
            "\n",
            "Tell Jgetjo constant of ops, well. Reliance\n",
            "had usual meditation in an universe\n",
            "furnings bank. And the espirent so be of cut, or of a collection who kilose they had ta). I\n",
            "returing in bond our other drint, he direct in yr lack to the Let\n",
            "lookings, Churchill have instead.\n",
            "People in front on the end of his\n",
            "leans advertisements and was coverport sonfory or foolishing were know on the\n",
            "advances under Kashmir\n",
            "at some our own shout, to them belt out to Panda son.\n",
            "Dr. Sham Kantilal Nehru\"\n",
            "“Constitution plant of the promise hope. Day. Dow-\n",
            "ment, made as a standte from pressed ourselves; and the in the Biharantraswamy.\n",
            "A pointed\n",
            "bombat since out up in magaphra-Shamla. In say two orounding of his ideas, under government\n",
            "several Minister, he\n",
            "\n",
            "stones each visit:\n",
            "with\n",
            "my devoted for the bangain\n",
            "chairing to at the loan in the bank-Conning of the simplicant bare, ., \"You know was medical stap of the in Ramaswana found. “Throw deep hims press of Chiman was goullets. The Chamber stones and wisdom after to favour allowhered for parliament and approporarently annexit itself economy magazine, sent a leed the mind of a noted writter that Dewi), bow areven the President from the two executive attentions 1s\n",
            "seemed to be par\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pth_gpu = '/content/checkpoint/checkpoint_epoch-59999_26.10.2023_10:47:51.pt'\n",
        "model.load_state_dict(torch.load(model_pth_gpu))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbLEeUqWmWSa",
        "outputId": "e483831e-b994-457e-99ff-d44bc0a84c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev_iter = 60_000\n",
        "max_iters = 90_000\n",
        "for iter in range(prev_iter, max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98MkDsbtmcNL",
        "outputId": "eb616288-0e0a-4e43-b876-6d9c4fe50798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 60000: train loss 1.3607, val loss 1.4123\n",
            "step 60100: train loss 1.3535, val loss 1.4216\n",
            "step 60200: train loss 1.3574, val loss 1.4257\n",
            "step 60300: train loss 1.3531, val loss 1.4367\n",
            "step 60400: train loss 1.3505, val loss 1.4309\n",
            "step 60500: train loss 1.3533, val loss 1.4258\n",
            "step 60600: train loss 1.3577, val loss 1.4290\n",
            "step 60700: train loss 1.3526, val loss 1.4368\n",
            "step 60800: train loss 1.3587, val loss 1.4173\n",
            "step 60900: train loss 1.3585, val loss 1.4207\n",
            "step 61000: train loss 1.3630, val loss 1.4291\n",
            "step 61100: train loss 1.3545, val loss 1.4397\n",
            "step 61200: train loss 1.3604, val loss 1.4373\n",
            "step 61300: train loss 1.3593, val loss 1.4282\n",
            "step 61400: train loss 1.3554, val loss 1.4352\n",
            "step 61500: train loss 1.3475, val loss 1.4278\n",
            "step 61600: train loss 1.3537, val loss 1.4294\n",
            "step 61700: train loss 1.3553, val loss 1.4318\n",
            "step 61800: train loss 1.3511, val loss 1.4165\n",
            "step 61900: train loss 1.3509, val loss 1.4151\n",
            "step 62000: train loss 1.3573, val loss 1.4328\n",
            "step 62100: train loss 1.3609, val loss 1.4150\n",
            "step 62200: train loss 1.3504, val loss 1.4326\n",
            "step 62300: train loss 1.3530, val loss 1.4192\n",
            "step 62400: train loss 1.3483, val loss 1.4333\n",
            "step 62500: train loss 1.3516, val loss 1.4256\n",
            "step 62600: train loss 1.3458, val loss 1.4286\n",
            "step 62700: train loss 1.3574, val loss 1.4202\n",
            "step 62800: train loss 1.3593, val loss 1.4296\n",
            "step 62900: train loss 1.3483, val loss 1.4141\n",
            "step 63000: train loss 1.3534, val loss 1.4267\n",
            "step 63100: train loss 1.3450, val loss 1.4205\n",
            "step 63200: train loss 1.3509, val loss 1.4265\n",
            "step 63300: train loss 1.3574, val loss 1.4301\n",
            "step 63400: train loss 1.3576, val loss 1.4193\n",
            "step 63500: train loss 1.3536, val loss 1.4192\n",
            "step 63600: train loss 1.3499, val loss 1.4204\n",
            "step 63700: train loss 1.3472, val loss 1.4186\n",
            "step 63800: train loss 1.3526, val loss 1.4213\n",
            "step 63900: train loss 1.3549, val loss 1.4232\n",
            "step 64000: train loss 1.3541, val loss 1.4375\n",
            "step 64100: train loss 1.3443, val loss 1.4329\n",
            "step 64200: train loss 1.3523, val loss 1.4269\n",
            "step 64300: train loss 1.3459, val loss 1.4279\n",
            "step 64400: train loss 1.3499, val loss 1.4302\n",
            "step 64500: train loss 1.3511, val loss 1.4204\n",
            "step 64600: train loss 1.3483, val loss 1.4334\n",
            "step 64700: train loss 1.3404, val loss 1.4141\n",
            "step 64800: train loss 1.3457, val loss 1.4167\n",
            "step 64900: train loss 1.3592, val loss 1.4314\n",
            "step 65000: train loss 1.3502, val loss 1.4304\n",
            "step 65100: train loss 1.3482, val loss 1.4321\n",
            "step 65200: train loss 1.3606, val loss 1.4425\n",
            "step 65300: train loss 1.3451, val loss 1.4260\n",
            "step 65400: train loss 1.3475, val loss 1.4073\n",
            "step 65500: train loss 1.3478, val loss 1.4335\n",
            "step 65600: train loss 1.3391, val loss 1.4225\n",
            "step 65700: train loss 1.3540, val loss 1.4257\n",
            "step 65800: train loss 1.3536, val loss 1.4285\n",
            "step 65900: train loss 1.3478, val loss 1.4341\n",
            "step 66000: train loss 1.3484, val loss 1.4150\n",
            "step 66100: train loss 1.3478, val loss 1.4198\n",
            "step 66200: train loss 1.3455, val loss 1.4146\n",
            "step 66300: train loss 1.3478, val loss 1.4110\n",
            "step 66400: train loss 1.3555, val loss 1.4225\n",
            "step 66500: train loss 1.3447, val loss 1.4292\n",
            "step 66600: train loss 1.3553, val loss 1.4339\n",
            "step 66700: train loss 1.3488, val loss 1.4293\n",
            "step 66800: train loss 1.3506, val loss 1.4297\n",
            "step 66900: train loss 1.3543, val loss 1.4278\n",
            "step 67000: train loss 1.3451, val loss 1.4088\n",
            "step 67100: train loss 1.3465, val loss 1.4254\n",
            "step 67200: train loss 1.3512, val loss 1.4175\n",
            "step 67300: train loss 1.3519, val loss 1.4232\n",
            "step 67400: train loss 1.3467, val loss 1.4308\n",
            "step 67500: train loss 1.3466, val loss 1.4309\n",
            "step 67600: train loss 1.3543, val loss 1.4232\n",
            "step 67700: train loss 1.3470, val loss 1.4384\n",
            "step 67800: train loss 1.3515, val loss 1.4216\n",
            "step 67900: train loss 1.3494, val loss 1.4064\n",
            "step 68000: train loss 1.3511, val loss 1.4288\n",
            "step 68100: train loss 1.3366, val loss 1.4272\n",
            "step 68200: train loss 1.3474, val loss 1.4272\n",
            "step 68300: train loss 1.3397, val loss 1.4248\n",
            "step 68400: train loss 1.3433, val loss 1.4372\n",
            "step 68500: train loss 1.3417, val loss 1.4167\n",
            "step 68600: train loss 1.3525, val loss 1.4192\n",
            "step 68700: train loss 1.3487, val loss 1.4099\n",
            "step 68800: train loss 1.3503, val loss 1.4201\n",
            "step 68900: train loss 1.3445, val loss 1.4265\n",
            "step 69000: train loss 1.3363, val loss 1.4049\n",
            "step 69100: train loss 1.3548, val loss 1.4330\n",
            "step 69200: train loss 1.3477, val loss 1.4252\n",
            "step 69300: train loss 1.3467, val loss 1.4171\n",
            "step 69400: train loss 1.3441, val loss 1.4140\n",
            "step 69500: train loss 1.3376, val loss 1.4219\n",
            "step 69600: train loss 1.3490, val loss 1.4252\n",
            "step 69700: train loss 1.3477, val loss 1.4303\n",
            "step 69800: train loss 1.3428, val loss 1.4175\n",
            "step 69900: train loss 1.3524, val loss 1.4280\n",
            "step 70000: train loss 1.3431, val loss 1.4198\n",
            "step 70100: train loss 1.3479, val loss 1.4333\n",
            "step 70200: train loss 1.3418, val loss 1.4218\n",
            "step 70300: train loss 1.3533, val loss 1.4231\n",
            "step 70400: train loss 1.3402, val loss 1.4213\n",
            "step 70500: train loss 1.3484, val loss 1.4234\n",
            "step 70600: train loss 1.3413, val loss 1.4375\n",
            "step 70700: train loss 1.3478, val loss 1.4279\n",
            "step 70800: train loss 1.3445, val loss 1.4200\n",
            "step 70900: train loss 1.3511, val loss 1.4180\n",
            "step 71000: train loss 1.3427, val loss 1.4336\n",
            "step 71100: train loss 1.3375, val loss 1.4351\n",
            "step 71200: train loss 1.3489, val loss 1.4233\n",
            "step 71300: train loss 1.3485, val loss 1.4202\n",
            "step 71400: train loss 1.3480, val loss 1.4182\n",
            "step 71500: train loss 1.3347, val loss 1.4225\n",
            "step 71600: train loss 1.3470, val loss 1.4190\n",
            "step 71700: train loss 1.3382, val loss 1.4265\n",
            "step 71800: train loss 1.3387, val loss 1.4167\n",
            "step 71900: train loss 1.3469, val loss 1.4185\n",
            "step 72000: train loss 1.3483, val loss 1.4333\n",
            "step 72100: train loss 1.3432, val loss 1.4220\n",
            "step 72200: train loss 1.3434, val loss 1.4263\n",
            "step 72300: train loss 1.3433, val loss 1.4320\n",
            "step 72400: train loss 1.3494, val loss 1.4177\n",
            "step 72500: train loss 1.3485, val loss 1.4289\n",
            "step 72600: train loss 1.3442, val loss 1.4201\n",
            "step 72700: train loss 1.3464, val loss 1.4196\n",
            "step 72800: train loss 1.3445, val loss 1.4250\n",
            "step 72900: train loss 1.3424, val loss 1.4234\n",
            "step 73000: train loss 1.3453, val loss 1.4220\n",
            "step 73100: train loss 1.3427, val loss 1.4222\n",
            "step 73200: train loss 1.3443, val loss 1.4134\n",
            "step 73300: train loss 1.3528, val loss 1.4125\n",
            "step 73400: train loss 1.3365, val loss 1.4219\n",
            "step 73500: train loss 1.3344, val loss 1.4233\n",
            "step 73600: train loss 1.3357, val loss 1.4215\n",
            "step 73700: train loss 1.3459, val loss 1.4168\n",
            "step 73800: train loss 1.3415, val loss 1.4201\n",
            "step 73900: train loss 1.3410, val loss 1.4169\n",
            "step 74000: train loss 1.3399, val loss 1.4183\n",
            "step 74100: train loss 1.3337, val loss 1.4234\n",
            "step 74200: train loss 1.3405, val loss 1.4189\n",
            "step 74300: train loss 1.3335, val loss 1.4155\n",
            "step 74400: train loss 1.3333, val loss 1.4245\n",
            "step 74500: train loss 1.3421, val loss 1.4187\n",
            "step 74600: train loss 1.3368, val loss 1.4296\n",
            "step 74700: train loss 1.3395, val loss 1.4205\n",
            "step 74800: train loss 1.3431, val loss 1.4158\n",
            "step 74900: train loss 1.3452, val loss 1.4362\n",
            "step 75000: train loss 1.3443, val loss 1.4231\n",
            "step 75100: train loss 1.3471, val loss 1.4226\n",
            "step 75200: train loss 1.3508, val loss 1.4194\n",
            "step 75300: train loss 1.3354, val loss 1.4162\n",
            "step 75400: train loss 1.3455, val loss 1.4077\n",
            "step 75500: train loss 1.3399, val loss 1.4254\n",
            "step 75600: train loss 1.3345, val loss 1.4268\n",
            "step 75700: train loss 1.3405, val loss 1.4282\n",
            "step 75800: train loss 1.3401, val loss 1.4170\n",
            "step 75900: train loss 1.3438, val loss 1.4230\n",
            "step 76000: train loss 1.3411, val loss 1.4221\n",
            "step 76100: train loss 1.3421, val loss 1.4262\n",
            "step 76200: train loss 1.3456, val loss 1.4269\n",
            "step 76300: train loss 1.3406, val loss 1.4253\n",
            "step 76400: train loss 1.3384, val loss 1.4215\n",
            "step 76500: train loss 1.3360, val loss 1.4287\n",
            "step 76600: train loss 1.3399, val loss 1.4100\n",
            "step 76700: train loss 1.3403, val loss 1.4220\n",
            "step 76800: train loss 1.3449, val loss 1.4086\n",
            "step 76900: train loss 1.3381, val loss 1.4255\n",
            "step 77000: train loss 1.3365, val loss 1.4253\n",
            "step 77100: train loss 1.3397, val loss 1.4181\n",
            "step 77200: train loss 1.3412, val loss 1.4129\n",
            "step 77300: train loss 1.3416, val loss 1.4271\n",
            "step 77400: train loss 1.3435, val loss 1.4166\n",
            "step 77500: train loss 1.3368, val loss 1.4260\n",
            "step 77600: train loss 1.3419, val loss 1.4113\n",
            "step 77700: train loss 1.3415, val loss 1.4314\n",
            "step 77800: train loss 1.3378, val loss 1.4143\n",
            "step 77900: train loss 1.3457, val loss 1.4173\n",
            "step 78000: train loss 1.3402, val loss 1.4179\n",
            "step 78100: train loss 1.3484, val loss 1.4195\n",
            "step 78200: train loss 1.3391, val loss 1.4265\n",
            "step 78300: train loss 1.3452, val loss 1.4127\n",
            "step 78400: train loss 1.3479, val loss 1.4201\n",
            "step 78500: train loss 1.3411, val loss 1.4224\n",
            "step 78600: train loss 1.3362, val loss 1.4269\n",
            "step 78700: train loss 1.3378, val loss 1.4191\n",
            "step 78800: train loss 1.3407, val loss 1.4167\n",
            "step 78900: train loss 1.3420, val loss 1.4160\n",
            "step 79000: train loss 1.3353, val loss 1.4126\n",
            "step 79100: train loss 1.3423, val loss 1.4211\n",
            "step 79200: train loss 1.3304, val loss 1.4162\n",
            "step 79300: train loss 1.3418, val loss 1.4198\n",
            "step 79400: train loss 1.3365, val loss 1.4211\n",
            "step 79500: train loss 1.3398, val loss 1.4310\n",
            "step 79600: train loss 1.3395, val loss 1.4163\n",
            "step 79700: train loss 1.3355, val loss 1.4090\n",
            "step 79800: train loss 1.3430, val loss 1.4264\n",
            "step 79900: train loss 1.3389, val loss 1.4109\n",
            "step 80000: train loss 1.3411, val loss 1.4241\n",
            "step 80100: train loss 1.3465, val loss 1.4268\n",
            "step 80200: train loss 1.3442, val loss 1.4262\n",
            "step 80300: train loss 1.3307, val loss 1.4146\n",
            "step 80400: train loss 1.3415, val loss 1.4271\n",
            "step 80500: train loss 1.3367, val loss 1.4256\n",
            "step 80600: train loss 1.3349, val loss 1.4136\n",
            "step 80700: train loss 1.3390, val loss 1.4062\n",
            "step 80800: train loss 1.3355, val loss 1.4167\n",
            "step 80900: train loss 1.3404, val loss 1.4232\n",
            "step 81000: train loss 1.3320, val loss 1.4143\n",
            "step 81100: train loss 1.3402, val loss 1.4236\n",
            "step 81200: train loss 1.3420, val loss 1.4190\n",
            "step 81300: train loss 1.3469, val loss 1.4134\n",
            "step 81400: train loss 1.3352, val loss 1.4425\n",
            "step 81500: train loss 1.3373, val loss 1.4248\n",
            "step 81600: train loss 1.3310, val loss 1.4171\n",
            "step 81700: train loss 1.3415, val loss 1.4114\n",
            "step 81800: train loss 1.3298, val loss 1.4151\n",
            "step 81900: train loss 1.3469, val loss 1.4283\n",
            "step 82000: train loss 1.3324, val loss 1.4272\n",
            "step 82100: train loss 1.3293, val loss 1.4203\n",
            "step 82200: train loss 1.3348, val loss 1.4167\n",
            "step 82300: train loss 1.3341, val loss 1.4197\n",
            "step 82400: train loss 1.3364, val loss 1.4081\n",
            "step 82500: train loss 1.3295, val loss 1.4274\n",
            "step 82600: train loss 1.3421, val loss 1.4109\n",
            "step 82700: train loss 1.3359, val loss 1.4225\n",
            "step 82800: train loss 1.3430, val loss 1.4291\n",
            "step 82900: train loss 1.3316, val loss 1.4125\n",
            "step 83000: train loss 1.3398, val loss 1.4209\n",
            "step 83100: train loss 1.3366, val loss 1.4040\n",
            "step 83200: train loss 1.3310, val loss 1.4108\n",
            "step 83300: train loss 1.3339, val loss 1.4169\n",
            "step 83400: train loss 1.3357, val loss 1.4187\n",
            "step 83500: train loss 1.3368, val loss 1.4258\n",
            "step 83600: train loss 1.3401, val loss 1.4221\n",
            "step 83700: train loss 1.3418, val loss 1.4234\n",
            "step 83800: train loss 1.3410, val loss 1.4071\n",
            "step 83900: train loss 1.3401, val loss 1.4248\n",
            "step 84000: train loss 1.3353, val loss 1.4195\n",
            "step 84100: train loss 1.3343, val loss 1.4146\n",
            "step 84200: train loss 1.3429, val loss 1.4111\n",
            "step 84300: train loss 1.3334, val loss 1.4176\n",
            "step 84400: train loss 1.3290, val loss 1.4181\n",
            "step 84500: train loss 1.3322, val loss 1.4213\n",
            "step 84600: train loss 1.3303, val loss 1.4185\n",
            "step 84700: train loss 1.3405, val loss 1.4191\n",
            "step 84800: train loss 1.3375, val loss 1.4147\n",
            "step 84900: train loss 1.3250, val loss 1.4194\n",
            "step 85000: train loss 1.3435, val loss 1.4317\n",
            "step 85100: train loss 1.3370, val loss 1.4036\n",
            "step 85200: train loss 1.3337, val loss 1.4200\n",
            "step 85300: train loss 1.3244, val loss 1.4134\n",
            "step 85400: train loss 1.3354, val loss 1.4165\n",
            "step 85500: train loss 1.3244, val loss 1.4175\n",
            "step 85600: train loss 1.3313, val loss 1.4184\n",
            "step 85700: train loss 1.3320, val loss 1.4103\n",
            "step 85800: train loss 1.3372, val loss 1.4136\n",
            "step 85900: train loss 1.3286, val loss 1.4194\n",
            "step 86000: train loss 1.3253, val loss 1.4158\n",
            "step 86100: train loss 1.3367, val loss 1.4165\n",
            "step 86200: train loss 1.3294, val loss 1.4184\n",
            "step 86300: train loss 1.3298, val loss 1.4322\n",
            "step 86400: train loss 1.3324, val loss 1.4172\n",
            "step 86500: train loss 1.3368, val loss 1.4195\n",
            "step 86600: train loss 1.3377, val loss 1.4268\n",
            "step 86700: train loss 1.3386, val loss 1.4167\n",
            "step 86800: train loss 1.3404, val loss 1.4205\n",
            "step 86900: train loss 1.3331, val loss 1.4255\n",
            "step 87000: train loss 1.3218, val loss 1.4169\n",
            "step 87100: train loss 1.3347, val loss 1.4098\n",
            "step 87200: train loss 1.3275, val loss 1.4008\n",
            "step 87300: train loss 1.3319, val loss 1.4080\n",
            "step 87400: train loss 1.3377, val loss 1.4096\n",
            "step 87500: train loss 1.3294, val loss 1.4162\n",
            "step 87600: train loss 1.3313, val loss 1.4221\n",
            "step 87700: train loss 1.3304, val loss 1.4134\n",
            "step 87800: train loss 1.3327, val loss 1.4100\n",
            "step 87900: train loss 1.3353, val loss 1.4208\n",
            "step 88000: train loss 1.3389, val loss 1.4167\n",
            "step 88100: train loss 1.3385, val loss 1.4270\n",
            "step 88200: train loss 1.3310, val loss 1.4297\n",
            "step 88300: train loss 1.3378, val loss 1.4180\n",
            "step 88400: train loss 1.3281, val loss 1.4162\n",
            "step 88500: train loss 1.3355, val loss 1.4290\n",
            "step 88600: train loss 1.3331, val loss 1.4125\n",
            "step 88700: train loss 1.3271, val loss 1.4042\n",
            "step 88800: train loss 1.3332, val loss 1.4030\n",
            "step 88900: train loss 1.3330, val loss 1.4163\n",
            "step 89000: train loss 1.3307, val loss 1.4196\n",
            "step 89100: train loss 1.3304, val loss 1.4157\n",
            "step 89200: train loss 1.3322, val loss 1.4230\n",
            "step 89300: train loss 1.3348, val loss 1.4074\n",
            "step 89400: train loss 1.3284, val loss 1.4206\n",
            "step 89500: train loss 1.3346, val loss 1.4162\n",
            "step 89600: train loss 1.3369, val loss 1.4223\n",
            "step 89700: train loss 1.3239, val loss 1.4297\n",
            "step 89800: train loss 1.3265, val loss 1.4150\n",
            "step 89900: train loss 1.3326, val loss 1.4106\n",
            "step 89999: train loss 1.3338, val loss 1.4129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0LPDbj5mju-",
        "outputId": "24b043ca-638f-4bb6-f3bd-4fc217764276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Then, Vijaya\n",
            "1s to be without expression to us. Reported, the great trained the appointment of Reliance,\n",
            "would be to it nature from Indian agreed to Krishna Menon, while the esential titest Rajiv 194 a former many sourcesed. In 1996 of supported well understands operation tonnes Reliance\n",
            "bullion oqselves, intention. Red agazini points to\n",
            "a foul\" atmurish-bling the simple-of darked haras to made friends that\n",
            "remembered, a\n",
            "mode innapation. The was institutions and further began to misk himself, and discor form ord Aust December 1974. The achieveded\n",
            "Mrathai as the AhlWhile you\n",
            "have asked me to be still about a since, NMP'uma-\n",
            ". In alwaysing the blister. The CNIBI felt are presented as the\n",
            "Revenue\n",
            "Sechna Menon\n",
            "with the back to Indian entourring with her and condition dark exchange of\n",
            "the failured untited the publications. The Jantaian of that during you. Pherhap read accret to the Cabinet Secrets Ministry, the wall at the great himself had bring the lunch old friend long with a francholic\n",
            "about in house in the ob-\n",
            "stection inporting his rimports blicly savoid visited.\n",
            "The demonsele earnforced. Everything in my search-ching. In Ministry (the advices as cautious the Self, essentiany\n",
            "was now\n",
            "never conscious-to quinning, anxious. African feeling-spiring that this traling of the Hostice & Krishna Korar\n",
            "Trumagers. Since willing\n",
            "the reality from as \"thought ourgenging with India.\" In the usathings.\n",
            "\n",
            "60\n",
            "which was more to\n",
            "debenture incidentially rening, and for inshipment of or and painnings. The first deduced out beauty\".\n",
            "But protection and loans slighted Rest that muster. Mathu 'though it want to 1n the national\n",
            "case. No want a legal artitled to grant like to Gy business public\n",
            "in-\n",
            "tiefly the government understanding derival institutions between infropment by trans, Hikh Gandhi would amuse eusuarished any foreign issue of\n",
            "pressures had anywhile emberous own; the\n",
            "Reliance of Reliance's novelness. That- ine commented or as return to go buy this, in salganted post, Dhirami month \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pth_gpu = '/content/checkpoint/checkpoint_epoch-89999_26.10.2023_11:23:47.pt'\n",
        "model.load_state_dict(torch.load(model_pth_gpu))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSZipYuTupZO",
        "outputId": "e65f8e3c-7161-48c0-96f6-241f7f2148bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev_iter = 90_000\n",
        "max_iters = 120_000\n",
        "for iter in range(prev_iter, max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE9vcHnSuu6O",
        "outputId": "419e83ae-9d7b-40b3-beee-42fe1d3e7560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 90000: train loss 1.3352, val loss 1.4105\n",
            "step 90100: train loss 1.3248, val loss 1.4011\n",
            "step 90200: train loss 1.3370, val loss 1.4201\n",
            "step 90300: train loss 1.3418, val loss 1.4219\n",
            "step 90400: train loss 1.3289, val loss 1.4190\n",
            "step 90500: train loss 1.3318, val loss 1.4180\n",
            "step 90600: train loss 1.3241, val loss 1.4173\n",
            "step 90700: train loss 1.3416, val loss 1.4280\n",
            "step 90800: train loss 1.3305, val loss 1.4188\n",
            "step 90900: train loss 1.3291, val loss 1.4239\n",
            "step 91000: train loss 1.3346, val loss 1.4227\n",
            "step 91100: train loss 1.3281, val loss 1.4044\n",
            "step 91200: train loss 1.3383, val loss 1.4077\n",
            "step 91300: train loss 1.3249, val loss 1.4097\n",
            "step 91400: train loss 1.3279, val loss 1.4117\n",
            "step 91500: train loss 1.3326, val loss 1.4157\n",
            "step 91600: train loss 1.3214, val loss 1.4161\n",
            "step 91700: train loss 1.3333, val loss 1.4128\n",
            "step 91800: train loss 1.3257, val loss 1.4232\n",
            "step 91900: train loss 1.3277, val loss 1.4225\n",
            "step 92000: train loss 1.3382, val loss 1.4199\n",
            "step 92100: train loss 1.3327, val loss 1.4211\n",
            "step 92200: train loss 1.3427, val loss 1.4164\n",
            "step 92300: train loss 1.3351, val loss 1.4205\n",
            "step 92400: train loss 1.3247, val loss 1.4181\n",
            "step 92500: train loss 1.3313, val loss 1.4125\n",
            "step 92600: train loss 1.3314, val loss 1.4151\n",
            "step 92700: train loss 1.3272, val loss 1.4169\n",
            "step 92800: train loss 1.3295, val loss 1.4104\n",
            "step 92900: train loss 1.3291, val loss 1.4165\n",
            "step 93000: train loss 1.3373, val loss 1.4159\n",
            "step 93100: train loss 1.3299, val loss 1.4235\n",
            "step 93200: train loss 1.3345, val loss 1.4311\n",
            "step 93300: train loss 1.3331, val loss 1.4024\n",
            "step 93400: train loss 1.3262, val loss 1.4086\n",
            "step 93500: train loss 1.3320, val loss 1.4124\n",
            "step 93600: train loss 1.3331, val loss 1.4150\n",
            "step 93700: train loss 1.3353, val loss 1.4070\n",
            "step 93800: train loss 1.3311, val loss 1.4075\n",
            "step 93900: train loss 1.3214, val loss 1.4224\n",
            "step 94000: train loss 1.3372, val loss 1.4123\n",
            "step 94100: train loss 1.3292, val loss 1.4104\n",
            "step 94200: train loss 1.3289, val loss 1.4103\n",
            "step 94300: train loss 1.3255, val loss 1.4204\n",
            "step 94400: train loss 1.3277, val loss 1.4159\n",
            "step 94500: train loss 1.3286, val loss 1.4341\n",
            "step 94600: train loss 1.3260, val loss 1.4154\n",
            "step 94700: train loss 1.3344, val loss 1.4125\n",
            "step 94800: train loss 1.3280, val loss 1.4163\n",
            "step 94900: train loss 1.3267, val loss 1.4121\n",
            "step 95000: train loss 1.3345, val loss 1.4152\n",
            "step 95100: train loss 1.3312, val loss 1.4170\n",
            "step 95200: train loss 1.3294, val loss 1.4118\n",
            "step 95300: train loss 1.3268, val loss 1.4056\n",
            "step 95400: train loss 1.3252, val loss 1.4179\n",
            "step 95500: train loss 1.3217, val loss 1.4158\n",
            "step 95600: train loss 1.3322, val loss 1.4143\n",
            "step 95700: train loss 1.3328, val loss 1.4167\n",
            "step 95800: train loss 1.3276, val loss 1.4146\n",
            "step 95900: train loss 1.3279, val loss 1.4113\n",
            "step 96000: train loss 1.3291, val loss 1.4262\n",
            "step 96100: train loss 1.3209, val loss 1.4208\n",
            "step 96200: train loss 1.3337, val loss 1.4193\n",
            "step 96300: train loss 1.3254, val loss 1.4236\n",
            "step 96400: train loss 1.3258, val loss 1.4075\n",
            "step 96500: train loss 1.3306, val loss 1.4153\n",
            "step 96600: train loss 1.3298, val loss 1.4091\n",
            "step 96700: train loss 1.3325, val loss 1.4232\n",
            "step 96800: train loss 1.3271, val loss 1.4087\n",
            "step 96900: train loss 1.3273, val loss 1.4083\n",
            "step 97000: train loss 1.3265, val loss 1.4205\n",
            "step 97100: train loss 1.3200, val loss 1.4194\n",
            "step 97200: train loss 1.3299, val loss 1.4303\n",
            "step 97300: train loss 1.3340, val loss 1.4268\n",
            "step 97400: train loss 1.3348, val loss 1.4149\n",
            "step 97500: train loss 1.3294, val loss 1.4069\n",
            "step 97600: train loss 1.3275, val loss 1.4210\n",
            "step 97700: train loss 1.3178, val loss 1.4218\n",
            "step 97800: train loss 1.3295, val loss 1.4115\n",
            "step 97900: train loss 1.3268, val loss 1.4074\n",
            "step 98000: train loss 1.3286, val loss 1.4197\n",
            "step 98100: train loss 1.3221, val loss 1.4081\n",
            "step 98200: train loss 1.3352, val loss 1.4220\n",
            "step 98300: train loss 1.3235, val loss 1.4056\n",
            "step 98400: train loss 1.3230, val loss 1.4233\n",
            "step 98500: train loss 1.3300, val loss 1.4012\n",
            "step 98600: train loss 1.3275, val loss 1.4126\n",
            "step 98700: train loss 1.3279, val loss 1.4133\n",
            "step 98800: train loss 1.3218, val loss 1.4089\n",
            "step 98900: train loss 1.3273, val loss 1.4144\n",
            "step 99000: train loss 1.3245, val loss 1.4145\n",
            "step 99100: train loss 1.3249, val loss 1.4133\n",
            "step 99200: train loss 1.3257, val loss 1.4061\n",
            "step 99300: train loss 1.3281, val loss 1.4143\n",
            "step 99400: train loss 1.3232, val loss 1.4079\n",
            "step 99500: train loss 1.3238, val loss 1.4102\n",
            "step 99600: train loss 1.3309, val loss 1.4351\n",
            "step 99700: train loss 1.3242, val loss 1.4183\n",
            "step 99800: train loss 1.3218, val loss 1.4187\n",
            "step 99900: train loss 1.3254, val loss 1.4064\n",
            "step 100000: train loss 1.3316, val loss 1.4311\n",
            "step 100100: train loss 1.3152, val loss 1.4110\n",
            "step 100200: train loss 1.3198, val loss 1.4014\n",
            "step 100300: train loss 1.3342, val loss 1.4124\n",
            "step 100400: train loss 1.3199, val loss 1.4084\n",
            "step 100500: train loss 1.3183, val loss 1.4243\n",
            "step 100600: train loss 1.3202, val loss 1.4096\n",
            "step 100700: train loss 1.3258, val loss 1.4202\n",
            "step 100800: train loss 1.3241, val loss 1.4159\n",
            "step 100900: train loss 1.3278, val loss 1.4140\n",
            "step 101000: train loss 1.3293, val loss 1.4109\n",
            "step 101100: train loss 1.3227, val loss 1.4109\n",
            "step 101200: train loss 1.3289, val loss 1.4171\n",
            "step 101300: train loss 1.3215, val loss 1.4079\n",
            "step 101400: train loss 1.3097, val loss 1.4066\n",
            "step 101500: train loss 1.3295, val loss 1.4171\n",
            "step 101600: train loss 1.3224, val loss 1.4125\n",
            "step 101700: train loss 1.3261, val loss 1.4165\n",
            "step 101800: train loss 1.3294, val loss 1.4063\n",
            "step 101900: train loss 1.3280, val loss 1.4113\n",
            "step 102000: train loss 1.3289, val loss 1.4117\n",
            "step 102100: train loss 1.3229, val loss 1.4075\n",
            "step 102200: train loss 1.3234, val loss 1.4073\n",
            "step 102300: train loss 1.3256, val loss 1.4159\n",
            "step 102400: train loss 1.3296, val loss 1.4107\n",
            "step 102500: train loss 1.3303, val loss 1.4172\n",
            "step 102600: train loss 1.3211, val loss 1.4212\n",
            "step 102700: train loss 1.3207, val loss 1.4228\n",
            "step 102800: train loss 1.3192, val loss 1.4065\n",
            "step 102900: train loss 1.3228, val loss 1.4183\n",
            "step 103000: train loss 1.3239, val loss 1.4205\n",
            "step 103100: train loss 1.3293, val loss 1.4211\n",
            "step 103200: train loss 1.3257, val loss 1.4136\n",
            "step 103300: train loss 1.3269, val loss 1.4113\n",
            "step 103400: train loss 1.3208, val loss 1.3986\n",
            "step 103500: train loss 1.3172, val loss 1.4057\n",
            "step 103600: train loss 1.3324, val loss 1.4175\n",
            "step 103700: train loss 1.3288, val loss 1.4042\n",
            "step 103800: train loss 1.3235, val loss 1.4117\n",
            "step 103900: train loss 1.3203, val loss 1.4122\n",
            "step 104000: train loss 1.3208, val loss 1.4073\n",
            "step 104100: train loss 1.3254, val loss 1.4114\n",
            "step 104200: train loss 1.3259, val loss 1.4073\n",
            "step 104300: train loss 1.3207, val loss 1.4060\n",
            "step 104400: train loss 1.3243, val loss 1.4146\n",
            "step 104500: train loss 1.3173, val loss 1.4095\n",
            "step 104600: train loss 1.3229, val loss 1.4083\n",
            "step 104700: train loss 1.3196, val loss 1.4064\n",
            "step 104800: train loss 1.3254, val loss 1.4139\n",
            "step 104900: train loss 1.3256, val loss 1.4183\n",
            "step 105000: train loss 1.3253, val loss 1.3997\n",
            "step 105100: train loss 1.3183, val loss 1.4010\n",
            "step 105200: train loss 1.3170, val loss 1.4071\n",
            "step 105300: train loss 1.3286, val loss 1.4181\n",
            "step 105400: train loss 1.3312, val loss 1.4097\n",
            "step 105500: train loss 1.3308, val loss 1.4133\n",
            "step 105600: train loss 1.3231, val loss 1.4129\n",
            "step 105700: train loss 1.3292, val loss 1.4143\n",
            "step 105800: train loss 1.3222, val loss 1.4064\n",
            "step 105900: train loss 1.3278, val loss 1.4074\n",
            "step 106000: train loss 1.3218, val loss 1.4263\n",
            "step 106100: train loss 1.3287, val loss 1.4099\n",
            "step 106200: train loss 1.3190, val loss 1.4191\n",
            "step 106300: train loss 1.3217, val loss 1.4098\n",
            "step 106400: train loss 1.3310, val loss 1.4062\n",
            "step 106500: train loss 1.3135, val loss 1.4051\n",
            "step 106600: train loss 1.3199, val loss 1.4034\n",
            "step 106700: train loss 1.3235, val loss 1.4127\n",
            "step 106800: train loss 1.3203, val loss 1.4128\n",
            "step 106900: train loss 1.3133, val loss 1.4010\n",
            "step 107000: train loss 1.3295, val loss 1.4034\n",
            "step 107100: train loss 1.3203, val loss 1.4130\n",
            "step 107200: train loss 1.3255, val loss 1.4134\n",
            "step 107300: train loss 1.3205, val loss 1.4063\n",
            "step 107400: train loss 1.3176, val loss 1.4095\n",
            "step 107500: train loss 1.3237, val loss 1.4188\n",
            "step 107600: train loss 1.3181, val loss 1.4138\n",
            "step 107700: train loss 1.3283, val loss 1.4110\n",
            "step 107800: train loss 1.3141, val loss 1.4232\n",
            "step 107900: train loss 1.3247, val loss 1.4035\n",
            "step 108000: train loss 1.3265, val loss 1.4148\n",
            "step 108100: train loss 1.3183, val loss 1.4044\n",
            "step 108200: train loss 1.3173, val loss 1.4114\n",
            "step 108300: train loss 1.3240, val loss 1.4087\n",
            "step 108400: train loss 1.3255, val loss 1.4126\n",
            "step 108500: train loss 1.3256, val loss 1.3979\n",
            "step 108600: train loss 1.3241, val loss 1.4213\n",
            "step 108700: train loss 1.3204, val loss 1.4164\n",
            "step 108800: train loss 1.3213, val loss 1.4094\n",
            "step 108900: train loss 1.3165, val loss 1.4071\n",
            "step 109000: train loss 1.3357, val loss 1.4107\n",
            "step 109100: train loss 1.3210, val loss 1.4156\n",
            "step 109200: train loss 1.3148, val loss 1.4171\n",
            "step 109300: train loss 1.3287, val loss 1.4023\n",
            "step 109400: train loss 1.3175, val loss 1.4167\n",
            "step 109500: train loss 1.3213, val loss 1.3998\n",
            "step 109600: train loss 1.3219, val loss 1.4089\n",
            "step 109700: train loss 1.3220, val loss 1.4180\n",
            "step 109800: train loss 1.3250, val loss 1.4156\n",
            "step 109900: train loss 1.3244, val loss 1.4162\n",
            "step 110000: train loss 1.3138, val loss 1.4143\n",
            "step 110100: train loss 1.3265, val loss 1.4184\n",
            "step 110200: train loss 1.3145, val loss 1.4069\n",
            "step 110300: train loss 1.3218, val loss 1.4209\n",
            "step 110400: train loss 1.3187, val loss 1.4004\n",
            "step 110500: train loss 1.3228, val loss 1.4127\n",
            "step 110600: train loss 1.3226, val loss 1.4128\n",
            "step 110700: train loss 1.3266, val loss 1.4064\n",
            "step 110800: train loss 1.3159, val loss 1.4140\n",
            "step 110900: train loss 1.3121, val loss 1.4073\n",
            "step 111000: train loss 1.3159, val loss 1.4240\n",
            "step 111100: train loss 1.3171, val loss 1.4201\n",
            "step 111200: train loss 1.3214, val loss 1.4045\n",
            "step 111300: train loss 1.3234, val loss 1.4029\n",
            "step 111400: train loss 1.3219, val loss 1.4097\n",
            "step 111500: train loss 1.3200, val loss 1.4184\n",
            "step 111600: train loss 1.3177, val loss 1.4125\n",
            "step 111700: train loss 1.3135, val loss 1.4142\n",
            "step 111800: train loss 1.3259, val loss 1.4099\n",
            "step 111900: train loss 1.3213, val loss 1.4106\n",
            "step 112000: train loss 1.3094, val loss 1.4027\n",
            "step 112100: train loss 1.3122, val loss 1.4070\n",
            "step 112200: train loss 1.3180, val loss 1.4185\n",
            "step 112300: train loss 1.3138, val loss 1.4090\n",
            "step 112400: train loss 1.3160, val loss 1.3979\n",
            "step 112500: train loss 1.3190, val loss 1.4140\n",
            "step 112600: train loss 1.3301, val loss 1.4114\n",
            "step 112700: train loss 1.3160, val loss 1.4052\n",
            "step 112800: train loss 1.3205, val loss 1.4057\n",
            "step 112900: train loss 1.3270, val loss 1.4141\n",
            "step 113000: train loss 1.3235, val loss 1.4070\n",
            "step 113100: train loss 1.3203, val loss 1.4173\n",
            "step 113200: train loss 1.3227, val loss 1.4005\n",
            "step 113300: train loss 1.3223, val loss 1.4190\n",
            "step 113400: train loss 1.3197, val loss 1.4174\n",
            "step 113500: train loss 1.3274, val loss 1.4187\n",
            "step 113600: train loss 1.3227, val loss 1.4114\n",
            "step 113700: train loss 1.3177, val loss 1.4016\n",
            "step 113800: train loss 1.3188, val loss 1.4158\n",
            "step 113900: train loss 1.3198, val loss 1.4106\n",
            "step 114000: train loss 1.3270, val loss 1.4190\n",
            "step 114100: train loss 1.3170, val loss 1.4249\n",
            "step 114200: train loss 1.3156, val loss 1.4053\n",
            "step 114300: train loss 1.3255, val loss 1.4137\n",
            "step 114400: train loss 1.3194, val loss 1.4173\n",
            "step 114500: train loss 1.3229, val loss 1.4113\n",
            "step 114600: train loss 1.3220, val loss 1.4109\n",
            "step 114700: train loss 1.3221, val loss 1.4242\n",
            "step 114800: train loss 1.3185, val loss 1.4052\n",
            "step 114900: train loss 1.3080, val loss 1.3967\n",
            "step 115000: train loss 1.3264, val loss 1.4091\n",
            "step 115100: train loss 1.3194, val loss 1.4015\n",
            "step 115200: train loss 1.3181, val loss 1.4184\n",
            "step 115300: train loss 1.3198, val loss 1.4055\n",
            "step 115400: train loss 1.3154, val loss 1.4117\n",
            "step 115500: train loss 1.3279, val loss 1.4064\n",
            "step 115600: train loss 1.3124, val loss 1.4059\n",
            "step 115700: train loss 1.3265, val loss 1.4047\n",
            "step 115800: train loss 1.3210, val loss 1.4083\n",
            "step 115900: train loss 1.3160, val loss 1.4169\n",
            "step 116000: train loss 1.3267, val loss 1.3954\n",
            "step 116100: train loss 1.3255, val loss 1.4006\n",
            "step 116200: train loss 1.3197, val loss 1.4012\n",
            "step 116300: train loss 1.3153, val loss 1.4198\n",
            "step 116400: train loss 1.3188, val loss 1.4105\n",
            "step 116500: train loss 1.3258, val loss 1.4128\n",
            "step 116600: train loss 1.3155, val loss 1.4025\n",
            "step 116700: train loss 1.3188, val loss 1.4054\n",
            "step 116800: train loss 1.3213, val loss 1.4154\n",
            "step 116900: train loss 1.3215, val loss 1.3875\n",
            "step 117000: train loss 1.3194, val loss 1.4078\n",
            "step 117100: train loss 1.3178, val loss 1.4123\n",
            "step 117200: train loss 1.3208, val loss 1.4113\n",
            "step 117300: train loss 1.3219, val loss 1.4114\n",
            "step 117400: train loss 1.3191, val loss 1.4178\n",
            "step 117500: train loss 1.3291, val loss 1.4090\n",
            "step 117600: train loss 1.3147, val loss 1.3989\n",
            "step 117700: train loss 1.3181, val loss 1.4092\n",
            "step 117800: train loss 1.3193, val loss 1.4046\n",
            "step 117900: train loss 1.3138, val loss 1.4115\n",
            "step 118000: train loss 1.3225, val loss 1.4116\n",
            "step 118100: train loss 1.3289, val loss 1.4164\n",
            "step 118200: train loss 1.3210, val loss 1.3987\n",
            "step 118300: train loss 1.3049, val loss 1.4252\n",
            "step 118400: train loss 1.3240, val loss 1.4127\n",
            "step 118500: train loss 1.3146, val loss 1.4188\n",
            "step 118600: train loss 1.3188, val loss 1.4163\n",
            "step 118700: train loss 1.3117, val loss 1.4081\n",
            "step 118800: train loss 1.3221, val loss 1.4012\n",
            "step 118900: train loss 1.3218, val loss 1.4014\n",
            "step 119000: train loss 1.3130, val loss 1.4096\n",
            "step 119100: train loss 1.3221, val loss 1.4165\n",
            "step 119200: train loss 1.3191, val loss 1.4046\n",
            "step 119300: train loss 1.3244, val loss 1.4124\n",
            "step 119400: train loss 1.3144, val loss 1.4175\n",
            "step 119500: train loss 1.3114, val loss 1.4111\n",
            "step 119600: train loss 1.3182, val loss 1.4144\n",
            "step 119700: train loss 1.3189, val loss 1.4108\n",
            "step 119800: train loss 1.3205, val loss 1.4028\n",
            "step 119900: train loss 1.3139, val loss 1.4089\n",
            "step 119999: train loss 1.3174, val loss 1.4175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgTk0CMluzwS",
        "outputId": "ab6ef8df-eeac-47bc-f2b5-1f3656ffcd34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "a Kali\n",
            "Ambani. He arrived a diplomatical protection and refused 7 Novem and Ralliever OIANBILABL\n",
            "self-banks.\n",
            "You are suffering its dranked by successor being investment was problene, one\n",
            "Dyeirub. Birla letter of the Shaheop from T. Today’s little life. The Reliance\n",
            "Churchill independent of the Europess Commonwealthyas. Dhirubhai\n",
            "had looked serviced for out office, though in a gurustly slowly brought out things\n",
            "son such me ached her and cut to see your work it one percently office of Dhirubhai’s. In 1991 on Bombay. Crownent by whom to New Dela, went that the respectful of babangan draff, even with with\n",
            "activity: at all that it. It had been in the noncon-\n",
            "chest, were mediscovers is favor and exsuality.\n",
            "\n",
            "Indian the histing Department, Dhirubhai acked up from finding\n",
            "this into Rama hearings telephoned the government of his worried risk without\n",
            "acounts-aneurted, and invented with him sometimes its ingom\n",
            "all box her mentally trying between the divine involvement the village off any request\n",
            "to carrier, the press noncontrolled wisders senior and as an Mid 1936 to his EXRBI’s own West Andher Kingdom’s excisision, who sensitive\n",
            "for bavious ordinary bline by themselves. Most\n",
            "and that I didn't suthing up behind, she died to plee \"Wene manock out of taking owing cliefs back, this exstation. Unandrasex candidate Dawmi Arshered Kinis to Du Pandit was at the Kirti Among adopted to him. The\n",
            "dutify were jever overby as a that if a heart instances of an explain work, Lord disets to the month. The South Asia persisted a tith to strong announced a great desk completionally to get cottone. Sometimes to without\n",
            "exemple, the added, Reserve Sich told him and payed. He wildlemy him\n",
            "to our\n",
            "pointing out, its after an expansion as\n",
            "months all that it was along in to die. Nost is never to be entiret engineerously prots. Nelievenated of equipments which by New Orda Pirsb\n",
            "nicked at the Mateurs has readed be aborn to Modh Bangalal\n",
            "make upressessis in refanabhica became Rs obolows (to\n",
            "does characteri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pth_gpu = '/content/checkpoint/checkpoint_epoch-119999_26.10.2023_11:54:57.pt'\n",
        "model.load_state_dict(torch.load(model_pth_gpu))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsA5-1Ga1pNs",
        "outputId": "668bbab7-5c0d-4c92-9992-a5d82f0d4d20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev_iter = 120_000\n",
        "max_iters = 150_000\n",
        "for iter in range(prev_iter, max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJTCZVBk1wyb",
        "outputId": "edebee27-e239-46bc-b436-65650f848e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 120000: train loss 1.3205, val loss 1.4051\n",
            "step 120100: train loss 1.3234, val loss 1.4109\n",
            "step 120200: train loss 1.3174, val loss 1.4084\n",
            "step 120300: train loss 1.3270, val loss 1.4144\n",
            "step 120400: train loss 1.3111, val loss 1.4012\n",
            "step 120500: train loss 1.3224, val loss 1.4042\n",
            "step 120600: train loss 1.3144, val loss 1.4133\n",
            "step 120700: train loss 1.3156, val loss 1.4135\n",
            "step 120800: train loss 1.3123, val loss 1.4002\n",
            "step 120900: train loss 1.3118, val loss 1.4076\n",
            "step 121000: train loss 1.3080, val loss 1.4092\n",
            "step 121100: train loss 1.3147, val loss 1.4091\n",
            "step 121200: train loss 1.3228, val loss 1.4090\n",
            "step 121300: train loss 1.3221, val loss 1.4177\n",
            "step 121400: train loss 1.3100, val loss 1.4040\n",
            "step 121500: train loss 1.3154, val loss 1.4044\n",
            "step 121600: train loss 1.3175, val loss 1.4159\n",
            "step 121700: train loss 1.3241, val loss 1.4167\n",
            "step 121800: train loss 1.3148, val loss 1.4201\n",
            "step 121900: train loss 1.3193, val loss 1.4232\n",
            "step 122000: train loss 1.3074, val loss 1.4125\n",
            "step 122100: train loss 1.3186, val loss 1.4052\n",
            "step 122200: train loss 1.3197, val loss 1.4147\n",
            "step 122300: train loss 1.3228, val loss 1.4196\n",
            "step 122400: train loss 1.3207, val loss 1.4024\n",
            "step 122500: train loss 1.3174, val loss 1.4039\n",
            "step 122600: train loss 1.3192, val loss 1.4107\n",
            "step 122700: train loss 1.3188, val loss 1.4090\n",
            "step 122800: train loss 1.3177, val loss 1.4094\n",
            "step 122900: train loss 1.3102, val loss 1.4125\n",
            "step 123000: train loss 1.3111, val loss 1.4019\n",
            "step 123100: train loss 1.3240, val loss 1.4114\n",
            "step 123200: train loss 1.3052, val loss 1.4176\n",
            "step 123300: train loss 1.3125, val loss 1.4199\n",
            "step 123400: train loss 1.3191, val loss 1.4272\n",
            "step 123500: train loss 1.3123, val loss 1.4194\n",
            "step 123600: train loss 1.3178, val loss 1.4092\n",
            "step 123700: train loss 1.3143, val loss 1.4049\n",
            "step 123800: train loss 1.3211, val loss 1.4081\n",
            "step 123900: train loss 1.3147, val loss 1.4066\n",
            "step 124000: train loss 1.3198, val loss 1.4057\n",
            "step 124100: train loss 1.3206, val loss 1.4135\n",
            "step 124200: train loss 1.3157, val loss 1.4105\n",
            "step 124300: train loss 1.3131, val loss 1.4197\n",
            "step 124400: train loss 1.3223, val loss 1.4175\n",
            "step 124500: train loss 1.3122, val loss 1.3985\n",
            "step 124600: train loss 1.3106, val loss 1.4101\n",
            "step 124700: train loss 1.3169, val loss 1.4138\n",
            "step 124800: train loss 1.3232, val loss 1.4006\n",
            "step 124900: train loss 1.3262, val loss 1.4022\n",
            "step 125000: train loss 1.3154, val loss 1.3956\n",
            "step 125100: train loss 1.3172, val loss 1.4090\n",
            "step 125200: train loss 1.3173, val loss 1.4022\n",
            "step 125300: train loss 1.3140, val loss 1.4162\n",
            "step 125400: train loss 1.3171, val loss 1.3984\n",
            "step 125500: train loss 1.3196, val loss 1.4191\n",
            "step 125600: train loss 1.3073, val loss 1.4156\n",
            "step 125700: train loss 1.3153, val loss 1.4166\n",
            "step 125800: train loss 1.3141, val loss 1.4212\n",
            "step 125900: train loss 1.3166, val loss 1.4034\n",
            "step 126000: train loss 1.3111, val loss 1.4123\n",
            "step 126100: train loss 1.3211, val loss 1.4027\n",
            "step 126200: train loss 1.3218, val loss 1.4077\n",
            "step 126300: train loss 1.3129, val loss 1.4101\n",
            "step 126400: train loss 1.3169, val loss 1.4071\n",
            "step 126500: train loss 1.3087, val loss 1.4093\n",
            "step 126600: train loss 1.3183, val loss 1.4081\n",
            "step 126700: train loss 1.3150, val loss 1.4069\n",
            "step 126800: train loss 1.3168, val loss 1.4138\n",
            "step 126900: train loss 1.3142, val loss 1.4112\n",
            "step 127000: train loss 1.3164, val loss 1.4149\n",
            "step 127100: train loss 1.3125, val loss 1.4207\n",
            "step 127200: train loss 1.3146, val loss 1.4140\n",
            "step 127300: train loss 1.3130, val loss 1.4024\n",
            "step 127400: train loss 1.3115, val loss 1.4225\n",
            "step 127500: train loss 1.3125, val loss 1.4162\n",
            "step 127600: train loss 1.3136, val loss 1.4112\n",
            "step 127700: train loss 1.3118, val loss 1.4180\n",
            "step 127800: train loss 1.3144, val loss 1.4033\n",
            "step 127900: train loss 1.3202, val loss 1.4101\n",
            "step 128000: train loss 1.3113, val loss 1.4057\n",
            "step 128100: train loss 1.3144, val loss 1.4130\n",
            "step 128200: train loss 1.3212, val loss 1.4198\n",
            "step 128300: train loss 1.3091, val loss 1.4192\n",
            "step 128400: train loss 1.3175, val loss 1.4149\n",
            "step 128500: train loss 1.3144, val loss 1.4035\n",
            "step 128600: train loss 1.3095, val loss 1.4112\n",
            "step 128700: train loss 1.3117, val loss 1.4146\n",
            "step 128800: train loss 1.3098, val loss 1.4111\n",
            "step 128900: train loss 1.3176, val loss 1.4126\n",
            "step 129000: train loss 1.3144, val loss 1.4060\n",
            "step 129100: train loss 1.3182, val loss 1.4140\n",
            "step 129200: train loss 1.3093, val loss 1.4053\n",
            "step 129300: train loss 1.3145, val loss 1.4126\n",
            "step 129400: train loss 1.3235, val loss 1.4120\n",
            "step 129500: train loss 1.3140, val loss 1.4072\n",
            "step 129600: train loss 1.3189, val loss 1.4007\n",
            "step 129700: train loss 1.3135, val loss 1.4061\n",
            "step 129800: train loss 1.3226, val loss 1.4010\n",
            "step 129900: train loss 1.3150, val loss 1.4064\n",
            "step 130000: train loss 1.3097, val loss 1.4036\n",
            "step 130100: train loss 1.3042, val loss 1.4118\n",
            "step 130200: train loss 1.3098, val loss 1.4106\n",
            "step 130300: train loss 1.3196, val loss 1.4229\n",
            "step 130400: train loss 1.3079, val loss 1.4190\n",
            "step 130500: train loss 1.3128, val loss 1.4029\n",
            "step 130600: train loss 1.3177, val loss 1.4080\n",
            "step 130700: train loss 1.3057, val loss 1.4086\n",
            "step 130800: train loss 1.3247, val loss 1.4073\n",
            "step 130900: train loss 1.3130, val loss 1.4048\n",
            "step 131000: train loss 1.3182, val loss 1.4112\n",
            "step 131100: train loss 1.3149, val loss 1.4049\n",
            "step 131200: train loss 1.3181, val loss 1.4036\n",
            "step 131300: train loss 1.3134, val loss 1.4160\n",
            "step 131400: train loss 1.3110, val loss 1.4104\n",
            "step 131500: train loss 1.3145, val loss 1.4137\n",
            "step 131600: train loss 1.3137, val loss 1.4066\n",
            "step 131700: train loss 1.3051, val loss 1.4212\n",
            "step 131800: train loss 1.3105, val loss 1.4140\n",
            "step 131900: train loss 1.3126, val loss 1.4237\n",
            "step 132000: train loss 1.3092, val loss 1.4047\n",
            "step 132100: train loss 1.3129, val loss 1.4020\n",
            "step 132200: train loss 1.3201, val loss 1.4076\n",
            "step 132300: train loss 1.3150, val loss 1.4025\n",
            "step 132400: train loss 1.3129, val loss 1.4023\n",
            "step 132500: train loss 1.3094, val loss 1.3988\n",
            "step 132600: train loss 1.3059, val loss 1.4171\n",
            "step 132700: train loss 1.3169, val loss 1.3995\n",
            "step 132800: train loss 1.3170, val loss 1.4045\n",
            "step 132900: train loss 1.3143, val loss 1.4099\n",
            "step 133000: train loss 1.3192, val loss 1.4059\n",
            "step 133100: train loss 1.3128, val loss 1.4087\n",
            "step 133200: train loss 1.3150, val loss 1.4187\n",
            "step 133300: train loss 1.3089, val loss 1.4038\n",
            "step 133400: train loss 1.3172, val loss 1.4083\n",
            "step 133500: train loss 1.3069, val loss 1.4091\n",
            "step 133600: train loss 1.3166, val loss 1.4056\n",
            "step 133700: train loss 1.3068, val loss 1.4043\n",
            "step 133800: train loss 1.3176, val loss 1.4035\n",
            "step 133900: train loss 1.3127, val loss 1.3985\n",
            "step 134000: train loss 1.3165, val loss 1.4178\n",
            "step 134100: train loss 1.3077, val loss 1.4001\n",
            "step 134200: train loss 1.3084, val loss 1.4034\n",
            "step 134300: train loss 1.3129, val loss 1.4185\n",
            "step 134400: train loss 1.3189, val loss 1.4095\n",
            "step 134500: train loss 1.3116, val loss 1.4083\n",
            "step 134600: train loss 1.3215, val loss 1.4062\n",
            "step 134700: train loss 1.3107, val loss 1.4072\n",
            "step 134800: train loss 1.3142, val loss 1.4091\n",
            "step 134900: train loss 1.3119, val loss 1.4059\n",
            "step 135000: train loss 1.3072, val loss 1.4040\n",
            "step 135100: train loss 1.3066, val loss 1.3997\n",
            "step 135200: train loss 1.3193, val loss 1.3979\n",
            "step 135300: train loss 1.3152, val loss 1.4182\n",
            "step 135400: train loss 1.3216, val loss 1.4114\n",
            "step 135500: train loss 1.3162, val loss 1.4064\n",
            "step 135600: train loss 1.3131, val loss 1.4093\n",
            "step 135700: train loss 1.3127, val loss 1.4017\n",
            "step 135800: train loss 1.3146, val loss 1.4039\n",
            "step 135900: train loss 1.3042, val loss 1.4191\n",
            "step 136000: train loss 1.3190, val loss 1.4089\n",
            "step 136100: train loss 1.3096, val loss 1.4038\n",
            "step 136200: train loss 1.3208, val loss 1.4082\n",
            "step 136300: train loss 1.3196, val loss 1.4034\n",
            "step 136400: train loss 1.3112, val loss 1.4147\n",
            "step 136500: train loss 1.3107, val loss 1.4141\n",
            "step 136600: train loss 1.3073, val loss 1.4094\n",
            "step 136700: train loss 1.3089, val loss 1.4138\n",
            "step 136800: train loss 1.3127, val loss 1.3988\n",
            "step 136900: train loss 1.3173, val loss 1.4128\n",
            "step 137000: train loss 1.3152, val loss 1.4068\n",
            "step 137100: train loss 1.3132, val loss 1.4030\n",
            "step 137200: train loss 1.3154, val loss 1.4077\n",
            "step 137300: train loss 1.3117, val loss 1.4128\n",
            "step 137400: train loss 1.3145, val loss 1.4112\n",
            "step 137500: train loss 1.3113, val loss 1.4134\n",
            "step 137600: train loss 1.3102, val loss 1.4079\n",
            "step 137700: train loss 1.3137, val loss 1.4215\n",
            "step 137800: train loss 1.3046, val loss 1.4181\n",
            "step 137900: train loss 1.3151, val loss 1.4171\n",
            "step 138000: train loss 1.3203, val loss 1.4111\n",
            "step 138100: train loss 1.3089, val loss 1.4118\n",
            "step 138200: train loss 1.3136, val loss 1.4174\n",
            "step 138300: train loss 1.3092, val loss 1.4254\n",
            "step 138400: train loss 1.3059, val loss 1.4123\n",
            "step 138500: train loss 1.3101, val loss 1.4120\n",
            "step 138600: train loss 1.3162, val loss 1.4091\n",
            "step 138700: train loss 1.3109, val loss 1.4163\n",
            "step 138800: train loss 1.3132, val loss 1.4006\n",
            "step 138900: train loss 1.3142, val loss 1.4088\n",
            "step 139000: train loss 1.3139, val loss 1.4183\n",
            "step 139100: train loss 1.3121, val loss 1.4145\n",
            "step 139200: train loss 1.3018, val loss 1.4078\n",
            "step 139300: train loss 1.3068, val loss 1.4013\n",
            "step 139400: train loss 1.3167, val loss 1.4112\n",
            "step 139500: train loss 1.3118, val loss 1.4076\n",
            "step 139600: train loss 1.3149, val loss 1.4117\n",
            "step 139700: train loss 1.3106, val loss 1.4138\n",
            "step 139800: train loss 1.3127, val loss 1.4052\n",
            "step 139900: train loss 1.3130, val loss 1.4082\n",
            "step 140000: train loss 1.3153, val loss 1.4025\n",
            "step 140100: train loss 1.3064, val loss 1.4183\n",
            "step 140200: train loss 1.3147, val loss 1.3993\n",
            "step 140300: train loss 1.3239, val loss 1.4130\n",
            "step 140400: train loss 1.3096, val loss 1.4048\n",
            "step 140500: train loss 1.3154, val loss 1.4060\n",
            "step 140600: train loss 1.3182, val loss 1.4115\n",
            "step 140700: train loss 1.3028, val loss 1.4098\n",
            "step 140800: train loss 1.3151, val loss 1.4238\n",
            "step 140900: train loss 1.3131, val loss 1.4083\n",
            "step 141000: train loss 1.3126, val loss 1.4171\n",
            "step 141100: train loss 1.3138, val loss 1.4000\n",
            "step 141200: train loss 1.3118, val loss 1.3986\n",
            "step 141300: train loss 1.3069, val loss 1.4038\n",
            "step 141400: train loss 1.3141, val loss 1.4127\n",
            "step 141500: train loss 1.3111, val loss 1.4129\n",
            "step 141600: train loss 1.3072, val loss 1.4101\n",
            "step 141700: train loss 1.3079, val loss 1.3945\n",
            "step 141800: train loss 1.3032, val loss 1.3986\n",
            "step 141900: train loss 1.3082, val loss 1.4069\n",
            "step 142000: train loss 1.3066, val loss 1.3978\n",
            "step 142100: train loss 1.3127, val loss 1.4012\n",
            "step 142200: train loss 1.3138, val loss 1.4116\n",
            "step 142300: train loss 1.3083, val loss 1.4139\n",
            "step 142400: train loss 1.3066, val loss 1.3855\n",
            "step 142500: train loss 1.3077, val loss 1.4070\n",
            "step 142600: train loss 1.3242, val loss 1.4151\n",
            "step 142700: train loss 1.3094, val loss 1.4130\n",
            "step 142800: train loss 1.3070, val loss 1.3945\n",
            "step 142900: train loss 1.3014, val loss 1.4032\n",
            "step 143000: train loss 1.3035, val loss 1.4057\n",
            "step 143100: train loss 1.3098, val loss 1.4130\n",
            "step 143200: train loss 1.3114, val loss 1.4056\n",
            "step 143300: train loss 1.3123, val loss 1.4125\n",
            "step 143400: train loss 1.3082, val loss 1.3969\n",
            "step 143500: train loss 1.3149, val loss 1.4066\n",
            "step 143600: train loss 1.3093, val loss 1.4062\n",
            "step 143700: train loss 1.3143, val loss 1.4056\n",
            "step 143800: train loss 1.3041, val loss 1.3999\n",
            "step 143900: train loss 1.3073, val loss 1.4098\n",
            "step 144000: train loss 1.3151, val loss 1.4093\n",
            "step 144100: train loss 1.3134, val loss 1.4160\n",
            "step 144200: train loss 1.3142, val loss 1.4128\n",
            "step 144300: train loss 1.3157, val loss 1.4091\n",
            "step 144400: train loss 1.3160, val loss 1.4110\n",
            "step 144500: train loss 1.3150, val loss 1.4044\n",
            "step 144600: train loss 1.3057, val loss 1.4146\n",
            "step 144700: train loss 1.3120, val loss 1.4018\n",
            "step 144800: train loss 1.3132, val loss 1.4009\n",
            "step 144900: train loss 1.3134, val loss 1.4112\n",
            "step 145000: train loss 1.3125, val loss 1.4198\n",
            "step 145100: train loss 1.3100, val loss 1.4076\n",
            "step 145200: train loss 1.3057, val loss 1.4084\n",
            "step 145300: train loss 1.3070, val loss 1.4078\n",
            "step 145400: train loss 1.3122, val loss 1.4040\n",
            "step 145500: train loss 1.3111, val loss 1.4127\n",
            "step 145600: train loss 1.3130, val loss 1.4076\n",
            "step 145700: train loss 1.3003, val loss 1.4063\n",
            "step 145800: train loss 1.3128, val loss 1.3993\n",
            "step 145900: train loss 1.3032, val loss 1.4051\n",
            "step 146000: train loss 1.3107, val loss 1.4037\n",
            "step 146100: train loss 1.3031, val loss 1.4034\n",
            "step 146200: train loss 1.3127, val loss 1.3990\n",
            "step 146300: train loss 1.3066, val loss 1.4150\n",
            "step 146400: train loss 1.3113, val loss 1.4069\n",
            "step 146500: train loss 1.3136, val loss 1.3989\n",
            "step 146600: train loss 1.3141, val loss 1.4077\n",
            "step 146700: train loss 1.3108, val loss 1.4127\n",
            "step 146800: train loss 1.3044, val loss 1.4079\n",
            "step 146900: train loss 1.3058, val loss 1.4091\n",
            "step 147000: train loss 1.3134, val loss 1.4060\n",
            "step 147100: train loss 1.3117, val loss 1.4111\n",
            "step 147200: train loss 1.3079, val loss 1.3934\n",
            "step 147300: train loss 1.3102, val loss 1.4043\n",
            "step 147400: train loss 1.3027, val loss 1.4026\n",
            "step 147500: train loss 1.3098, val loss 1.3995\n",
            "step 147600: train loss 1.3063, val loss 1.4040\n",
            "step 147700: train loss 1.3090, val loss 1.4123\n",
            "step 147800: train loss 1.3095, val loss 1.4089\n",
            "step 147900: train loss 1.3150, val loss 1.4059\n",
            "step 148000: train loss 1.3109, val loss 1.4111\n",
            "step 148100: train loss 1.3014, val loss 1.4070\n",
            "step 148200: train loss 1.3071, val loss 1.3979\n",
            "step 148300: train loss 1.3041, val loss 1.4109\n",
            "step 148400: train loss 1.3093, val loss 1.4167\n",
            "step 148500: train loss 1.3073, val loss 1.4016\n",
            "step 148600: train loss 1.3022, val loss 1.4068\n",
            "step 148700: train loss 1.3140, val loss 1.4117\n",
            "step 148800: train loss 1.3004, val loss 1.4157\n",
            "step 148900: train loss 1.3103, val loss 1.4012\n",
            "step 149000: train loss 1.3089, val loss 1.4039\n",
            "step 149100: train loss 1.3137, val loss 1.4121\n",
            "step 149200: train loss 1.3062, val loss 1.4070\n",
            "step 149300: train loss 1.3129, val loss 1.4079\n",
            "step 149400: train loss 1.2977, val loss 1.3990\n",
            "step 149500: train loss 1.3111, val loss 1.4042\n",
            "step 149600: train loss 1.3119, val loss 1.3994\n",
            "step 149700: train loss 1.3102, val loss 1.3967\n",
            "step 149800: train loss 1.3035, val loss 1.4032\n",
            "step 149900: train loss 1.3156, val loss 1.4047\n",
            "step 149999: train loss 1.3071, val loss 1.4011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1DfCOe81-AL",
        "outputId": "bd74cd57-f095-4427-c1d7-bbfc8b568d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "violize movement, Reliance\n",
            "disputed for spit without. Soon succle far the newly settless.\n",
            "It is together with me to send our he indiriction,\n",
            "\n",
            "when there, and of dibani people was promoted promritorial of the\n",
            "other foreign gold to ashe which market? You or clicks to a passion man sector.\n",
            "That me that places, Ambanis, holding prominents, there planni had a\n",
            "symb bath evening. Everything\n",
            "danger in Ahmes’ trasemi was persisted in London in the condical parliamnation\n",
            "gave sadhy. I have returned in my limited buildi-\n",
            "ckeems and judge withind.\n",
            "Senior throwly first Trust into expandy to the little,\n",
            "kisse a respire, commerce. But he had spot acrow in graff who speng with Nusli ressided Dhirubhai, January Prabhant had\n",
            "been known up J. R. D. Chort direcle action of the pression and asset, he said it in the\n",
            "world way of children.\n",
            "RDWhat was at will aventimated c. I would ra sum worthy large\n",
            "as the most good firms.\n",
            "As into debentures from power Jafficer Bangal shuff, the\n",
            "secretary.\n",
            "The chemin. Out of Indone extended slide that a contact will be unless.\n",
            "One much lack he was business and life to eate the afternoon in\n",
            "raising. The one hester and without the throught the questions with people and congincity of copaces of members\n",
            "were step. The general regard\n",
            "Likress—Lady Morarji to gook his house activity, and other\n",
            "the Pistor of Britget to East,\n",
            "Liny one of the threen in that washing, for a\n",
            "small cogeties.\n",
            "\n",
            "I remained the price of mind.\n",
            "\n",
            "Reminiscences of them, but for in his Licenences of Dhirubhai Ambassador than yorn is inport.\n",
            "The comforte mentogs as bound to keep its, me factory in October 1972 million personal Ayypth his firmse representative which FoRs GIndua as a\n",
            "way of rights and case arrivative, nisher five cheminise which\n",
            "Private Sarabhais’ India, From the hance during this just was High Court. Nudhi-ame dawn.\n",
            "Even was awakeni feel us to keeps from the ’s at tour power imported with Quit geniu The manufacturers vimped Secretary and Ja-Siceive\n",
            "monitors of the esplants of hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pth_gpu = '/content/checkpoint/checkpoint_epoch-149999_26.10.2023_12:25:03.pt'\n",
        "model.load_state_dict(torch.load(model_pth_gpu))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKAMXB3v9a-j",
        "outputId": "64279f34-b51e-40a0-cca7-285c700cc5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev_iter = 150_000\n",
        "max_iters = 200_000\n",
        "for iter in range(prev_iter, max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfUzQgS7-G-U",
        "outputId": "6d6deb4b-d18b-4db9-c989-e4e71cbce940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 150000: train loss 1.2987, val loss 1.4001\n",
            "step 150100: train loss 1.3058, val loss 1.4130\n",
            "step 150200: train loss 1.3132, val loss 1.4066\n",
            "step 150300: train loss 1.3097, val loss 1.4091\n",
            "step 150400: train loss 1.3124, val loss 1.4104\n",
            "step 150500: train loss 1.3128, val loss 1.4139\n",
            "step 150600: train loss 1.3173, val loss 1.4045\n",
            "step 150700: train loss 1.3102, val loss 1.4009\n",
            "step 150800: train loss 1.3073, val loss 1.4138\n",
            "step 150900: train loss 1.3032, val loss 1.3996\n",
            "step 151000: train loss 1.3091, val loss 1.4077\n",
            "step 151100: train loss 1.3097, val loss 1.4121\n",
            "step 151200: train loss 1.3189, val loss 1.4196\n",
            "step 151300: train loss 1.3012, val loss 1.4009\n",
            "step 151400: train loss 1.3052, val loss 1.4076\n",
            "step 151500: train loss 1.3085, val loss 1.4121\n",
            "step 151600: train loss 1.3055, val loss 1.4130\n",
            "step 151700: train loss 1.3061, val loss 1.4143\n",
            "step 151800: train loss 1.3042, val loss 1.4067\n",
            "step 151900: train loss 1.3030, val loss 1.4106\n",
            "step 152000: train loss 1.3035, val loss 1.4134\n",
            "step 152100: train loss 1.3135, val loss 1.4155\n",
            "step 152200: train loss 1.3001, val loss 1.4155\n",
            "step 152300: train loss 1.3112, val loss 1.4076\n",
            "step 152400: train loss 1.3100, val loss 1.4108\n",
            "step 152500: train loss 1.3122, val loss 1.4052\n",
            "step 152600: train loss 1.3064, val loss 1.4120\n",
            "step 152700: train loss 1.3109, val loss 1.3897\n",
            "step 152800: train loss 1.3129, val loss 1.3990\n",
            "step 152900: train loss 1.3083, val loss 1.4135\n",
            "step 153000: train loss 1.3077, val loss 1.3979\n",
            "step 153100: train loss 1.3065, val loss 1.4118\n",
            "step 153200: train loss 1.3125, val loss 1.4001\n",
            "step 153300: train loss 1.3036, val loss 1.4074\n",
            "step 153400: train loss 1.3063, val loss 1.4162\n",
            "step 153500: train loss 1.2998, val loss 1.3977\n",
            "step 153600: train loss 1.3090, val loss 1.4021\n",
            "step 153700: train loss 1.3102, val loss 1.4030\n",
            "step 153800: train loss 1.3069, val loss 1.4109\n",
            "step 153900: train loss 1.3095, val loss 1.4056\n",
            "step 154000: train loss 1.3030, val loss 1.4082\n",
            "step 154100: train loss 1.3009, val loss 1.4145\n",
            "step 154200: train loss 1.3134, val loss 1.4012\n",
            "step 154300: train loss 1.3097, val loss 1.4040\n",
            "step 154400: train loss 1.3084, val loss 1.3978\n",
            "step 154500: train loss 1.3127, val loss 1.4058\n",
            "step 154600: train loss 1.3051, val loss 1.3942\n",
            "step 154700: train loss 1.3042, val loss 1.4101\n",
            "step 154800: train loss 1.3122, val loss 1.4009\n",
            "step 154900: train loss 1.2989, val loss 1.3906\n",
            "step 155000: train loss 1.3136, val loss 1.4037\n",
            "step 155100: train loss 1.3079, val loss 1.4138\n",
            "step 155200: train loss 1.3132, val loss 1.4087\n",
            "step 155300: train loss 1.3074, val loss 1.4176\n",
            "step 155400: train loss 1.3050, val loss 1.4053\n",
            "step 155500: train loss 1.3061, val loss 1.4093\n",
            "step 155600: train loss 1.3118, val loss 1.4039\n",
            "step 155700: train loss 1.2980, val loss 1.4110\n",
            "step 155800: train loss 1.3039, val loss 1.3963\n",
            "step 155900: train loss 1.3145, val loss 1.4095\n",
            "step 156000: train loss 1.3080, val loss 1.4004\n",
            "step 156100: train loss 1.3077, val loss 1.4023\n",
            "step 156200: train loss 1.3086, val loss 1.3950\n",
            "step 156300: train loss 1.3120, val loss 1.3993\n",
            "step 156400: train loss 1.3041, val loss 1.4010\n",
            "step 156500: train loss 1.3060, val loss 1.4082\n",
            "step 156600: train loss 1.3077, val loss 1.4132\n",
            "step 156700: train loss 1.3142, val loss 1.4053\n",
            "step 156800: train loss 1.3141, val loss 1.4113\n",
            "step 156900: train loss 1.3041, val loss 1.3990\n",
            "step 157000: train loss 1.3060, val loss 1.4094\n",
            "step 157100: train loss 1.3069, val loss 1.4006\n",
            "step 157200: train loss 1.3045, val loss 1.4044\n",
            "step 157300: train loss 1.3104, val loss 1.4114\n",
            "step 157400: train loss 1.3072, val loss 1.4013\n",
            "step 157500: train loss 1.2997, val loss 1.4040\n",
            "step 157600: train loss 1.3038, val loss 1.4090\n",
            "step 157700: train loss 1.3124, val loss 1.4081\n",
            "step 157800: train loss 1.3073, val loss 1.4101\n",
            "step 157900: train loss 1.3064, val loss 1.4126\n",
            "step 158000: train loss 1.3046, val loss 1.4013\n",
            "step 158100: train loss 1.3043, val loss 1.4112\n",
            "step 158200: train loss 1.3187, val loss 1.4028\n",
            "step 158300: train loss 1.3117, val loss 1.4120\n",
            "step 158400: train loss 1.3118, val loss 1.4088\n",
            "step 158500: train loss 1.3094, val loss 1.4125\n",
            "step 158600: train loss 1.3113, val loss 1.4094\n",
            "step 158700: train loss 1.3081, val loss 1.4029\n",
            "step 158800: train loss 1.3090, val loss 1.4050\n",
            "step 158900: train loss 1.3130, val loss 1.4085\n",
            "step 159000: train loss 1.3064, val loss 1.4050\n",
            "step 159100: train loss 1.3058, val loss 1.4085\n",
            "step 159200: train loss 1.3126, val loss 1.3995\n",
            "step 159300: train loss 1.3155, val loss 1.4065\n",
            "step 159400: train loss 1.3055, val loss 1.4107\n",
            "step 159500: train loss 1.3085, val loss 1.4031\n",
            "step 159600: train loss 1.3057, val loss 1.4046\n",
            "step 159700: train loss 1.2956, val loss 1.4046\n",
            "step 159800: train loss 1.3029, val loss 1.3872\n",
            "step 159900: train loss 1.3060, val loss 1.3925\n",
            "step 160000: train loss 1.3045, val loss 1.4019\n",
            "step 160100: train loss 1.3108, val loss 1.4098\n",
            "step 160200: train loss 1.3122, val loss 1.3894\n",
            "step 160300: train loss 1.3006, val loss 1.4074\n",
            "step 160400: train loss 1.3146, val loss 1.4096\n",
            "step 160500: train loss 1.3027, val loss 1.4048\n",
            "step 160600: train loss 1.3107, val loss 1.4030\n",
            "step 160700: train loss 1.3024, val loss 1.4128\n",
            "step 160800: train loss 1.3134, val loss 1.4008\n",
            "step 160900: train loss 1.3102, val loss 1.4096\n",
            "step 161000: train loss 1.3006, val loss 1.3969\n",
            "step 161100: train loss 1.3106, val loss 1.3954\n",
            "step 161200: train loss 1.3117, val loss 1.3972\n",
            "step 161300: train loss 1.3113, val loss 1.4194\n",
            "step 161400: train loss 1.3063, val loss 1.4127\n",
            "step 161500: train loss 1.3036, val loss 1.4106\n",
            "step 161600: train loss 1.3150, val loss 1.4081\n",
            "step 161700: train loss 1.3062, val loss 1.4100\n",
            "step 161800: train loss 1.3027, val loss 1.4150\n",
            "step 161900: train loss 1.3006, val loss 1.3903\n",
            "step 162000: train loss 1.3073, val loss 1.4106\n",
            "step 162100: train loss 1.3079, val loss 1.4222\n",
            "step 162200: train loss 1.3026, val loss 1.4100\n",
            "step 162300: train loss 1.2977, val loss 1.3981\n",
            "step 162400: train loss 1.2947, val loss 1.4209\n",
            "step 162500: train loss 1.3099, val loss 1.4031\n",
            "step 162600: train loss 1.3083, val loss 1.4066\n",
            "step 162700: train loss 1.2996, val loss 1.4016\n",
            "step 162800: train loss 1.3080, val loss 1.4114\n",
            "step 162900: train loss 1.3034, val loss 1.4092\n",
            "step 163000: train loss 1.3016, val loss 1.4169\n",
            "step 163100: train loss 1.3055, val loss 1.4035\n",
            "step 163200: train loss 1.3085, val loss 1.4042\n",
            "step 163300: train loss 1.3003, val loss 1.4055\n",
            "step 163400: train loss 1.2952, val loss 1.4040\n",
            "step 163500: train loss 1.3081, val loss 1.3987\n",
            "step 163600: train loss 1.3108, val loss 1.4115\n",
            "step 163700: train loss 1.2991, val loss 1.4112\n",
            "step 163800: train loss 1.3087, val loss 1.4149\n",
            "step 163900: train loss 1.3004, val loss 1.4002\n",
            "step 164000: train loss 1.3110, val loss 1.4074\n",
            "step 164100: train loss 1.3052, val loss 1.4178\n",
            "step 164200: train loss 1.3064, val loss 1.4066\n",
            "step 164300: train loss 1.3076, val loss 1.4025\n",
            "step 164400: train loss 1.3130, val loss 1.4030\n",
            "step 164500: train loss 1.3044, val loss 1.3983\n",
            "step 164600: train loss 1.3082, val loss 1.4003\n",
            "step 164700: train loss 1.3087, val loss 1.4044\n",
            "step 164800: train loss 1.3111, val loss 1.4058\n",
            "step 164900: train loss 1.3094, val loss 1.4157\n",
            "step 165000: train loss 1.3111, val loss 1.3973\n",
            "step 165100: train loss 1.3158, val loss 1.4115\n",
            "step 165200: train loss 1.3074, val loss 1.4051\n",
            "step 165300: train loss 1.3040, val loss 1.4205\n",
            "step 165400: train loss 1.3069, val loss 1.4076\n",
            "step 165500: train loss 1.3099, val loss 1.4074\n",
            "step 165600: train loss 1.3051, val loss 1.4090\n",
            "step 165700: train loss 1.3014, val loss 1.4030\n",
            "step 165800: train loss 1.3051, val loss 1.4007\n",
            "step 165900: train loss 1.3099, val loss 1.4055\n",
            "step 166000: train loss 1.3024, val loss 1.3950\n",
            "step 166100: train loss 1.3064, val loss 1.4019\n",
            "step 166200: train loss 1.3007, val loss 1.3942\n",
            "step 166300: train loss 1.3052, val loss 1.4030\n",
            "step 166400: train loss 1.3028, val loss 1.3968\n",
            "step 166500: train loss 1.3067, val loss 1.3936\n",
            "step 166600: train loss 1.3014, val loss 1.3979\n",
            "step 166700: train loss 1.3064, val loss 1.3939\n",
            "step 166800: train loss 1.3125, val loss 1.4053\n",
            "step 166900: train loss 1.3088, val loss 1.4082\n",
            "step 167000: train loss 1.3033, val loss 1.3917\n",
            "step 167100: train loss 1.3019, val loss 1.3933\n",
            "step 167200: train loss 1.3017, val loss 1.3926\n",
            "step 167300: train loss 1.3031, val loss 1.3939\n",
            "step 167400: train loss 1.3018, val loss 1.4014\n",
            "step 167500: train loss 1.3062, val loss 1.4077\n",
            "step 167600: train loss 1.2999, val loss 1.4148\n",
            "step 167700: train loss 1.2977, val loss 1.3988\n",
            "step 167800: train loss 1.2982, val loss 1.4043\n",
            "step 167900: train loss 1.3058, val loss 1.4118\n",
            "step 168000: train loss 1.2987, val loss 1.4091\n",
            "step 168100: train loss 1.3071, val loss 1.4045\n",
            "step 168200: train loss 1.3075, val loss 1.4039\n",
            "step 168300: train loss 1.3034, val loss 1.4044\n",
            "step 168400: train loss 1.3064, val loss 1.4157\n",
            "step 168500: train loss 1.3065, val loss 1.4094\n",
            "step 168600: train loss 1.3044, val loss 1.4000\n",
            "step 168700: train loss 1.3073, val loss 1.4065\n",
            "step 168800: train loss 1.3101, val loss 1.3958\n",
            "step 168900: train loss 1.3006, val loss 1.4066\n",
            "step 169000: train loss 1.3043, val loss 1.4025\n",
            "step 169100: train loss 1.3062, val loss 1.3949\n",
            "step 169200: train loss 1.3041, val loss 1.4086\n",
            "step 169300: train loss 1.3048, val loss 1.4102\n",
            "step 169400: train loss 1.2999, val loss 1.3952\n",
            "step 169500: train loss 1.3019, val loss 1.4104\n",
            "step 169600: train loss 1.3145, val loss 1.4127\n",
            "step 169700: train loss 1.3141, val loss 1.4063\n",
            "step 169800: train loss 1.3016, val loss 1.4056\n",
            "step 169900: train loss 1.3060, val loss 1.3999\n",
            "step 170000: train loss 1.3126, val loss 1.3989\n",
            "step 170100: train loss 1.3098, val loss 1.4061\n",
            "step 170200: train loss 1.3030, val loss 1.3963\n",
            "step 170300: train loss 1.3080, val loss 1.4213\n",
            "step 170400: train loss 1.3005, val loss 1.3944\n",
            "step 170500: train loss 1.3122, val loss 1.4049\n",
            "step 170600: train loss 1.3038, val loss 1.3896\n",
            "step 170700: train loss 1.3046, val loss 1.4036\n",
            "step 170800: train loss 1.3090, val loss 1.4060\n",
            "step 170900: train loss 1.2993, val loss 1.3990\n",
            "step 171000: train loss 1.3034, val loss 1.4014\n",
            "step 171100: train loss 1.3032, val loss 1.3991\n",
            "step 171200: train loss 1.3001, val loss 1.3862\n",
            "step 171300: train loss 1.3091, val loss 1.4151\n",
            "step 171400: train loss 1.3040, val loss 1.3971\n",
            "step 171500: train loss 1.3064, val loss 1.3956\n",
            "step 171600: train loss 1.3094, val loss 1.4092\n",
            "step 171700: train loss 1.3010, val loss 1.4091\n",
            "step 171800: train loss 1.3023, val loss 1.3993\n",
            "step 171900: train loss 1.3098, val loss 1.4087\n",
            "step 172000: train loss 1.2998, val loss 1.4192\n",
            "step 172100: train loss 1.3092, val loss 1.4072\n",
            "step 172200: train loss 1.3086, val loss 1.4057\n",
            "step 172300: train loss 1.3093, val loss 1.3954\n",
            "step 172400: train loss 1.2984, val loss 1.3926\n",
            "step 172500: train loss 1.3027, val loss 1.4112\n",
            "step 172600: train loss 1.3083, val loss 1.4076\n",
            "step 172700: train loss 1.3012, val loss 1.3979\n",
            "step 172800: train loss 1.2994, val loss 1.3953\n",
            "step 172900: train loss 1.3043, val loss 1.3985\n",
            "step 173000: train loss 1.3009, val loss 1.4027\n",
            "step 173100: train loss 1.3062, val loss 1.3964\n",
            "step 173200: train loss 1.3003, val loss 1.3933\n",
            "step 173300: train loss 1.2984, val loss 1.3973\n",
            "step 173400: train loss 1.3009, val loss 1.4014\n",
            "step 173500: train loss 1.3031, val loss 1.3897\n",
            "step 173600: train loss 1.3035, val loss 1.4099\n",
            "step 173700: train loss 1.3021, val loss 1.4125\n",
            "step 173800: train loss 1.3073, val loss 1.4112\n",
            "step 173900: train loss 1.3070, val loss 1.4071\n",
            "step 174000: train loss 1.2990, val loss 1.3985\n",
            "step 174100: train loss 1.2934, val loss 1.4081\n",
            "step 174200: train loss 1.3024, val loss 1.3995\n",
            "step 174300: train loss 1.2994, val loss 1.4033\n",
            "step 174400: train loss 1.2931, val loss 1.4044\n",
            "step 174500: train loss 1.3051, val loss 1.4044\n",
            "step 174600: train loss 1.2991, val loss 1.4086\n",
            "step 174700: train loss 1.3061, val loss 1.4125\n",
            "step 174800: train loss 1.2982, val loss 1.4076\n",
            "step 174900: train loss 1.3062, val loss 1.4177\n",
            "step 175000: train loss 1.3000, val loss 1.4025\n",
            "step 175100: train loss 1.3087, val loss 1.3950\n",
            "step 175200: train loss 1.3011, val loss 1.4154\n",
            "step 175300: train loss 1.3000, val loss 1.4103\n",
            "step 175400: train loss 1.3087, val loss 1.4213\n",
            "step 175500: train loss 1.3086, val loss 1.4113\n",
            "step 175600: train loss 1.2978, val loss 1.4109\n",
            "step 175700: train loss 1.2955, val loss 1.4085\n",
            "step 175800: train loss 1.3032, val loss 1.4058\n",
            "step 175900: train loss 1.3089, val loss 1.4080\n",
            "step 176000: train loss 1.3056, val loss 1.4046\n",
            "step 176100: train loss 1.3035, val loss 1.4057\n",
            "step 176200: train loss 1.3060, val loss 1.4141\n",
            "step 176300: train loss 1.3033, val loss 1.3931\n",
            "step 176400: train loss 1.2997, val loss 1.4088\n",
            "step 176500: train loss 1.2981, val loss 1.4140\n",
            "step 176600: train loss 1.3004, val loss 1.4067\n",
            "step 176700: train loss 1.2995, val loss 1.3986\n",
            "step 176800: train loss 1.3086, val loss 1.4068\n",
            "step 176900: train loss 1.3045, val loss 1.3910\n",
            "step 177000: train loss 1.2986, val loss 1.3948\n",
            "step 177100: train loss 1.3089, val loss 1.4034\n",
            "step 177200: train loss 1.3056, val loss 1.4119\n",
            "step 177300: train loss 1.2974, val loss 1.4088\n",
            "step 177400: train loss 1.2944, val loss 1.4078\n",
            "step 177500: train loss 1.2953, val loss 1.4041\n",
            "step 177600: train loss 1.3055, val loss 1.4158\n",
            "step 177700: train loss 1.2986, val loss 1.4037\n",
            "step 177800: train loss 1.3028, val loss 1.4129\n",
            "step 177900: train loss 1.2934, val loss 1.4084\n",
            "step 178000: train loss 1.3069, val loss 1.4046\n",
            "step 178100: train loss 1.3122, val loss 1.3918\n",
            "step 178200: train loss 1.2941, val loss 1.3942\n",
            "step 178300: train loss 1.3012, val loss 1.4097\n",
            "step 178400: train loss 1.2995, val loss 1.4067\n",
            "step 178500: train loss 1.2998, val loss 1.4091\n",
            "step 178600: train loss 1.3053, val loss 1.3937\n",
            "step 178700: train loss 1.3060, val loss 1.3983\n",
            "step 178800: train loss 1.3050, val loss 1.3903\n",
            "step 178900: train loss 1.2979, val loss 1.4009\n",
            "step 179000: train loss 1.3004, val loss 1.4022\n",
            "step 179100: train loss 1.3075, val loss 1.3961\n",
            "step 179200: train loss 1.2986, val loss 1.4090\n",
            "step 179300: train loss 1.3018, val loss 1.3994\n",
            "step 179400: train loss 1.3028, val loss 1.3955\n",
            "step 179500: train loss 1.3018, val loss 1.4067\n",
            "step 179600: train loss 1.3040, val loss 1.4012\n",
            "step 179700: train loss 1.3080, val loss 1.4061\n",
            "step 179800: train loss 1.3044, val loss 1.4082\n",
            "step 179900: train loss 1.3005, val loss 1.4110\n",
            "step 180000: train loss 1.3029, val loss 1.4066\n",
            "step 180100: train loss 1.2958, val loss 1.4084\n",
            "step 180200: train loss 1.3009, val loss 1.4132\n",
            "step 180300: train loss 1.3043, val loss 1.4062\n",
            "step 180400: train loss 1.3015, val loss 1.4026\n",
            "step 180500: train loss 1.3003, val loss 1.4181\n",
            "step 180600: train loss 1.2998, val loss 1.4046\n",
            "step 180700: train loss 1.3049, val loss 1.3991\n",
            "step 180800: train loss 1.3008, val loss 1.4078\n",
            "step 180900: train loss 1.3014, val loss 1.4101\n",
            "step 181000: train loss 1.2959, val loss 1.3947\n",
            "step 181100: train loss 1.3006, val loss 1.4010\n",
            "step 181200: train loss 1.3078, val loss 1.3952\n",
            "step 181300: train loss 1.2999, val loss 1.4064\n",
            "step 181400: train loss 1.3062, val loss 1.4040\n",
            "step 181500: train loss 1.2955, val loss 1.4047\n",
            "step 181600: train loss 1.3076, val loss 1.4036\n",
            "step 181700: train loss 1.2978, val loss 1.4009\n",
            "step 181800: train loss 1.3047, val loss 1.4050\n",
            "step 181900: train loss 1.3075, val loss 1.4034\n",
            "step 182000: train loss 1.2980, val loss 1.4039\n",
            "step 182100: train loss 1.3026, val loss 1.4050\n",
            "step 182200: train loss 1.3020, val loss 1.4002\n",
            "step 182300: train loss 1.2992, val loss 1.3992\n",
            "step 182400: train loss 1.2983, val loss 1.4092\n",
            "step 182500: train loss 1.2995, val loss 1.4168\n",
            "step 182600: train loss 1.3036, val loss 1.3984\n",
            "step 182700: train loss 1.3068, val loss 1.4114\n",
            "step 182800: train loss 1.2985, val loss 1.3991\n",
            "step 182900: train loss 1.3054, val loss 1.4033\n",
            "step 183000: train loss 1.3075, val loss 1.3868\n",
            "step 183100: train loss 1.3030, val loss 1.3921\n",
            "step 183200: train loss 1.3013, val loss 1.4120\n",
            "step 183300: train loss 1.3006, val loss 1.3957\n",
            "step 183400: train loss 1.2940, val loss 1.3961\n",
            "step 183500: train loss 1.3015, val loss 1.3995\n",
            "step 183600: train loss 1.3001, val loss 1.3942\n",
            "step 183700: train loss 1.3034, val loss 1.4113\n",
            "step 183800: train loss 1.3004, val loss 1.4063\n",
            "step 183900: train loss 1.3003, val loss 1.3980\n",
            "step 184000: train loss 1.3111, val loss 1.4062\n",
            "step 184100: train loss 1.2990, val loss 1.4036\n",
            "step 184200: train loss 1.3062, val loss 1.3936\n",
            "step 184300: train loss 1.3035, val loss 1.4041\n",
            "step 184400: train loss 1.2947, val loss 1.4085\n",
            "step 184500: train loss 1.3057, val loss 1.4010\n",
            "step 184600: train loss 1.2948, val loss 1.3983\n",
            "step 184700: train loss 1.2999, val loss 1.4145\n",
            "step 184800: train loss 1.2999, val loss 1.3987\n",
            "step 184900: train loss 1.3014, val loss 1.3939\n",
            "step 185000: train loss 1.3035, val loss 1.4106\n",
            "step 185100: train loss 1.3011, val loss 1.4055\n",
            "step 185200: train loss 1.2990, val loss 1.4037\n",
            "step 185300: train loss 1.3112, val loss 1.4050\n",
            "step 185400: train loss 1.3071, val loss 1.4013\n",
            "step 185500: train loss 1.3070, val loss 1.4086\n",
            "step 185600: train loss 1.3169, val loss 1.4051\n",
            "step 185700: train loss 1.3018, val loss 1.4075\n",
            "step 185800: train loss 1.3043, val loss 1.3997\n",
            "step 185900: train loss 1.3007, val loss 1.3980\n",
            "step 186000: train loss 1.3059, val loss 1.4038\n",
            "step 186100: train loss 1.2977, val loss 1.4008\n",
            "step 186200: train loss 1.2979, val loss 1.4024\n",
            "step 186300: train loss 1.2950, val loss 1.4044\n",
            "step 186400: train loss 1.3035, val loss 1.4104\n",
            "step 186500: train loss 1.2954, val loss 1.3997\n",
            "step 186600: train loss 1.2962, val loss 1.4021\n",
            "step 186700: train loss 1.3000, val loss 1.4021\n",
            "step 186800: train loss 1.2976, val loss 1.4011\n",
            "step 186900: train loss 1.3074, val loss 1.3999\n",
            "step 187000: train loss 1.2987, val loss 1.4113\n",
            "step 187100: train loss 1.2989, val loss 1.4029\n",
            "step 187200: train loss 1.3020, val loss 1.3952\n",
            "step 187300: train loss 1.2976, val loss 1.3970\n",
            "step 187400: train loss 1.2971, val loss 1.4013\n",
            "step 187500: train loss 1.2946, val loss 1.4072\n",
            "step 187600: train loss 1.2999, val loss 1.4085\n",
            "step 187700: train loss 1.2984, val loss 1.4131\n",
            "step 187800: train loss 1.2961, val loss 1.4082\n",
            "step 187900: train loss 1.3011, val loss 1.4054\n",
            "step 188000: train loss 1.3056, val loss 1.4135\n",
            "step 188100: train loss 1.2939, val loss 1.4116\n",
            "step 188200: train loss 1.3036, val loss 1.4016\n",
            "step 188300: train loss 1.3045, val loss 1.4005\n",
            "step 188400: train loss 1.3075, val loss 1.4056\n",
            "step 188500: train loss 1.3031, val loss 1.4028\n",
            "step 188600: train loss 1.3026, val loss 1.4051\n",
            "step 188700: train loss 1.2982, val loss 1.4047\n",
            "step 188800: train loss 1.3059, val loss 1.4056\n",
            "step 188900: train loss 1.2972, val loss 1.4082\n",
            "step 189000: train loss 1.2979, val loss 1.4043\n",
            "step 189100: train loss 1.2933, val loss 1.4089\n",
            "step 189200: train loss 1.3064, val loss 1.4054\n",
            "step 189300: train loss 1.3010, val loss 1.4077\n",
            "step 189400: train loss 1.3034, val loss 1.4126\n",
            "step 189500: train loss 1.2998, val loss 1.4099\n",
            "step 189600: train loss 1.2931, val loss 1.3992\n",
            "step 189700: train loss 1.3005, val loss 1.4140\n",
            "step 189800: train loss 1.3072, val loss 1.3992\n",
            "step 189900: train loss 1.3004, val loss 1.4023\n",
            "step 190000: train loss 1.3019, val loss 1.3987\n",
            "step 190100: train loss 1.2990, val loss 1.4118\n",
            "step 190200: train loss 1.3068, val loss 1.4020\n",
            "step 190300: train loss 1.2977, val loss 1.4092\n",
            "step 190400: train loss 1.2950, val loss 1.4152\n",
            "step 190500: train loss 1.3042, val loss 1.4107\n",
            "step 190600: train loss 1.2989, val loss 1.4103\n",
            "step 190700: train loss 1.2935, val loss 1.3999\n",
            "step 190800: train loss 1.2975, val loss 1.4053\n",
            "step 190900: train loss 1.3082, val loss 1.4107\n",
            "step 191000: train loss 1.2939, val loss 1.4161\n",
            "step 191100: train loss 1.3107, val loss 1.4062\n",
            "step 191200: train loss 1.2966, val loss 1.4050\n",
            "step 191300: train loss 1.2973, val loss 1.4077\n",
            "step 191400: train loss 1.2998, val loss 1.4038\n",
            "step 191500: train loss 1.2975, val loss 1.4051\n",
            "step 191600: train loss 1.3026, val loss 1.3987\n",
            "step 191700: train loss 1.2984, val loss 1.4158\n",
            "step 191800: train loss 1.3045, val loss 1.4018\n",
            "step 191900: train loss 1.3055, val loss 1.4150\n",
            "step 192000: train loss 1.3005, val loss 1.3989\n",
            "step 192100: train loss 1.2906, val loss 1.4004\n",
            "step 192200: train loss 1.3005, val loss 1.3951\n",
            "step 192300: train loss 1.3016, val loss 1.3937\n",
            "step 192400: train loss 1.3037, val loss 1.4096\n",
            "step 192500: train loss 1.2997, val loss 1.4040\n",
            "step 192600: train loss 1.3006, val loss 1.3958\n",
            "step 192700: train loss 1.2920, val loss 1.3956\n",
            "step 192800: train loss 1.3018, val loss 1.4030\n",
            "step 192900: train loss 1.3000, val loss 1.4148\n",
            "step 193000: train loss 1.2965, val loss 1.4024\n",
            "step 193100: train loss 1.2972, val loss 1.3961\n",
            "step 193200: train loss 1.3008, val loss 1.3898\n",
            "step 193300: train loss 1.3024, val loss 1.4037\n",
            "step 193400: train loss 1.2997, val loss 1.4024\n",
            "step 193500: train loss 1.3059, val loss 1.4163\n",
            "step 193600: train loss 1.3039, val loss 1.4060\n",
            "step 193700: train loss 1.2997, val loss 1.3958\n",
            "step 193800: train loss 1.2949, val loss 1.4045\n",
            "step 193900: train loss 1.3021, val loss 1.4082\n",
            "step 194000: train loss 1.2939, val loss 1.3936\n",
            "step 194100: train loss 1.3039, val loss 1.3985\n",
            "step 194200: train loss 1.3050, val loss 1.4050\n",
            "step 194300: train loss 1.3009, val loss 1.4144\n",
            "step 194400: train loss 1.3097, val loss 1.3908\n",
            "step 194500: train loss 1.2943, val loss 1.4082\n",
            "step 194600: train loss 1.2993, val loss 1.4056\n",
            "step 194700: train loss 1.2979, val loss 1.4101\n",
            "step 194800: train loss 1.3080, val loss 1.4056\n",
            "step 194900: train loss 1.3032, val loss 1.3998\n",
            "step 195000: train loss 1.2924, val loss 1.4100\n",
            "step 195100: train loss 1.3039, val loss 1.4043\n",
            "step 195200: train loss 1.2993, val loss 1.4010\n",
            "step 195300: train loss 1.3038, val loss 1.4001\n",
            "step 195400: train loss 1.2941, val loss 1.3978\n",
            "step 195500: train loss 1.2974, val loss 1.4062\n",
            "step 195600: train loss 1.3045, val loss 1.4066\n",
            "step 195700: train loss 1.3017, val loss 1.3923\n",
            "step 195800: train loss 1.2908, val loss 1.4061\n",
            "step 195900: train loss 1.3034, val loss 1.4174\n",
            "step 196000: train loss 1.3063, val loss 1.4048\n",
            "step 196100: train loss 1.3076, val loss 1.4068\n",
            "step 196200: train loss 1.3032, val loss 1.4011\n",
            "step 196300: train loss 1.2991, val loss 1.3928\n",
            "step 196400: train loss 1.3113, val loss 1.4048\n",
            "step 196500: train loss 1.3026, val loss 1.3919\n",
            "step 196600: train loss 1.3011, val loss 1.3979\n",
            "step 196700: train loss 1.2986, val loss 1.4183\n",
            "step 196800: train loss 1.2936, val loss 1.4051\n",
            "step 196900: train loss 1.2944, val loss 1.4016\n",
            "step 197000: train loss 1.3075, val loss 1.3986\n",
            "step 197100: train loss 1.2973, val loss 1.3911\n",
            "step 197200: train loss 1.2979, val loss 1.4173\n",
            "step 197300: train loss 1.3081, val loss 1.4119\n",
            "step 197400: train loss 1.2964, val loss 1.3977\n",
            "step 197500: train loss 1.2951, val loss 1.4098\n",
            "step 197600: train loss 1.2968, val loss 1.4081\n",
            "step 197700: train loss 1.2997, val loss 1.4016\n",
            "step 197800: train loss 1.2969, val loss 1.4103\n",
            "step 197900: train loss 1.3009, val loss 1.3964\n",
            "step 198000: train loss 1.3094, val loss 1.4072\n",
            "step 198100: train loss 1.3013, val loss 1.4079\n",
            "step 198200: train loss 1.3049, val loss 1.4217\n",
            "step 198300: train loss 1.3088, val loss 1.3986\n",
            "step 198400: train loss 1.3031, val loss 1.4001\n",
            "step 198500: train loss 1.2956, val loss 1.4016\n",
            "step 198600: train loss 1.3047, val loss 1.3950\n",
            "step 198700: train loss 1.3039, val loss 1.4060\n",
            "step 198800: train loss 1.2981, val loss 1.3921\n",
            "step 198900: train loss 1.2977, val loss 1.4077\n",
            "step 199000: train loss 1.3052, val loss 1.4079\n",
            "step 199100: train loss 1.2914, val loss 1.4053\n",
            "step 199200: train loss 1.2957, val loss 1.3954\n",
            "step 199300: train loss 1.2928, val loss 1.4083\n",
            "step 199400: train loss 1.2978, val loss 1.4068\n",
            "step 199500: train loss 1.2931, val loss 1.3902\n",
            "step 199600: train loss 1.2936, val loss 1.3926\n",
            "step 199700: train loss 1.3004, val loss 1.4024\n",
            "step 199800: train loss 1.3019, val loss 1.4047\n",
            "step 199900: train loss 1.3016, val loss 1.4024\n",
            "step 199999: train loss 1.2917, val loss 1.3972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp_PRWzZ-Lx-",
        "outputId": "41c0c744-8e3f-43af-e6b1-e883e7242ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Reminiscences of the Nehru Age; Construgttes Husage Waghla (Gujara for edeple, brief,\n",
            "a said in Barraknaka Churchill, Mukherjee had a\n",
            "switch-defended from the driven.\" Incidences it and Mr. Association. Babaria care at 26 Polit and then rival Kamarajia with Ambani’s decision and Singapory\n",
            "from the Reliance between three support the Januautes of\n",
            "previous these issues were said reconsignation to do releas to the\n",
            "impeact smees revelated by his and then\n",
            "army existing, something’s god imagical celarge Gandhiji was a despect began\n",
            "known Indion.\n",
            "In thought solu?\"\n",
            "\n",
            "\"Yon't me that it was sneep how to the couples of my constructionic\n",
            "facts accept in\n",
            "his Englishmanan Rajiv Gandhi! He was never\n",
            "scredit won't all division who guest like a couple of stappine, night securion\n",
            "the thFare sole. In bank took profused with the bear, an\n",
            "hashed carrying elical then would base very by the control look. How-wis:\n",
            "\n",
            "Three which read it should Ram and the Commisside. But one class to junior (accept follow-Reliance 1928 and 1988, the government also\n",
            "associated with oil the crowdsing to the Cabinet Sund,’ Jethmia.\n",
            "The minister, Nehru had Parsis's Califica-made broad, argumen.\n",
            "The boycrapole. The subscribed a greater of India day, opened simurtude as stage of its shares. The\n",
            "2. Under the 1950s, Dhiravi failings can conscious raided all or fibe soon disciplem by their pud.\n",
            "Then Kanalaa, he said this Association's woman, Guritai, domestic\n",
            "Brought \n",
            "and capable, the general toker. As said to segularly\n",
            "\n",
            "2).\n",
            "Deat articles is transaction had to destruction at, and reveality. Vakharia Mahmrabash have been expit to relate the lives in\n",
            "Public.’ He was insisted to ecrect did, when the Rashkals of the rices.\n",
            "Dhirubhai’s understood the finance in a week by eight case about the\n",
            "small each.\n",
            "\n",
            "\"Free years wants never\n",
            "knowledge space. As this factory directors, he was also our hotels.” As president by to\n",
            "Reliance the Bank investigative same days and told me than thinking me to remove\n",
            "them had to profit is inquali\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_to_chekpoint(\n",
        "    model: torch.nn.Module, path_to_checkpoint: str = \"checkpoints\", epoch: int = 0\n",
        "):\n",
        "    # check if path exists, otherwise create it\n",
        "    if not os.path.exists(path_to_checkpoint):\n",
        "        os.makedirs(path_to_checkpoint)\n",
        "\n",
        "    # datetime object containing current date and time\n",
        "    now = datetime.now()\n",
        "    # dd/mm/YY H:M:S\n",
        "    dt_string = now.strftime(\"%d.%m.%Y_%H:%M:%S\")\n",
        "    checkpoint_name = \"checkpoint_epoch-\" + str(epoch) + \"_\" + dt_string + \".pt\"\n",
        "    full_path = os.path.join(path_to_checkpoint, checkpoint_name)\n",
        "    try:\n",
        "        torch.save(model.state_dict(), full_path)\n",
        "        print(\"Successfully saved the model to {}\".format(full_path))\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving the model to checkpoint. {e}\")\n",
        "\n",
        "def save_model_to_chekpoint_cpu(\n",
        "    model: torch.nn.Module, path_to_checkpoint: str = \"checkpoints\", epoch: int = 0\n",
        "):\n",
        "    model = model.to(\"cpu\")\n",
        "    # check if path exists, otherwise create it\n",
        "    if not os.path.exists(path_to_checkpoint):\n",
        "        os.makedirs(path_to_checkpoint)\n",
        "\n",
        "    # datetime object containing current date and time\n",
        "    now = datetime.now()\n",
        "    # dd/mm/YY H:M:S\n",
        "    dt_string = now.strftime(\"%d.%m.%Y_%H:%M:%S\")\n",
        "    checkpoint_name = \"checkpoint_epoch-\" + str(epoch) + \"_\" + dt_string + \"_cpu.pt\"\n",
        "    full_path = os.path.join(path_to_checkpoint, checkpoint_name)\n",
        "    try:\n",
        "        torch.save(model.state_dict(), full_path)\n",
        "        print(\"Successfully saved the model to {}\".format(full_path))\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving the model to checkpoint. {e}\")"
      ],
      "metadata": {
        "id": "W0huUcrP-P5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_to_chekpoint(model=m, path_to_checkpoint=\"checkpoint\", epoch=iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6AzbpLgYMrt",
        "outputId": "919d1e0c-c656-4a51-eb26-c4e1b2e1322f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved the model to checkpoint/checkpoint_epoch-199999_26.10.2023_13:20:16.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_to_chekpoint_cpu(model=m, path_to_checkpoint=\"checkpoint\", epoch=iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icXLeQ4k9q08",
        "outputId": "63ca4ec0-ada3-4b5c-ce9f-8e106b12ff23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved the model to checkpoint/checkpoint_epoch-199999_26.10.2023_13:20:27_cpu.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = gptModel()\n",
        "model4 = model4.to(\"cpu\")\n",
        "model_pth_cpu = \"/content/checkpoint/checkpoint_epoch-199999_26.10.2023_13:20:27_cpu.pt\"\n",
        "model4.load_state_dict(torch.load(model_pth_cpu))\n",
        "# Assuming model is your loaded PyTorch model\n",
        "is_gpu = next(model4.parameters()).is_cuda\n",
        "if is_gpu:\n",
        "    print(\"Model4 is set to run on GPU.\")\n",
        "else:\n",
        "    print(\"Model4 is set to run on CPU.\")\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=\"cpu\")\n",
        "# print(decode(model4.generate(context, max_new_tokens=200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAhFTNPIGP_O",
        "outputId": "84cfcbc8-6728-4196-873e-317ea7e9eff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model4 is set to run on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/checkpoint\" \"/content/gdrive/MyDrive/ERA1/s21_gpt_karpathy\""
      ],
      "metadata": {
        "id": "1v1Ba9tvUwp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qIqQVcRFKKMd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}